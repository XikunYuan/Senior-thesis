{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf('k3b.gdf', preload=True, stim_channel='auto')\n",
    "\n",
    "# print(raw.ch_names)\n",
    "\n",
    "# events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "# picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "# tmin, tmax = 1., 4.\n",
    "\n",
    "\n",
    "# event_id = [ 768, 1791, 2323, 2324, 2325, 2326, 2337, 2339, 3346, 3347, 3348, 3349, 3360, 3362]\n",
    "# epochs = mne.Epochs(raw, events, None, tmin, tmax, proj=True, picks=picks,\n",
    "#                 baseline=None, preload=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.ch_names[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "data = raw.get_data()\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "# raw = mne.io.read_raw_edf(\"k3b.gdf\", preload=True)\n",
    "_, pos, kind, chan, dur = raw.find_edf_events()\n",
    "kind_hex = np.array([hex(n) for n in kind])\n",
    "\n",
    "print(pos[1])\n",
    "print(pos[2])\n",
    "print(pos[:20])\n",
    "\n",
    "print(len(kind_hex))\n",
    "print(len(pos))\n",
    "sampling_rate = 250\n",
    "\n",
    "\n",
    "beep_pos = [pos[i] for i in range(len(kind_hex)) if kind_hex[i] in ['0x301','0x302','0x303','0x304']]\n",
    "labels = [i[4] for i in kind_hex if i in ['0x301','0x302','0x303','0x304']]\n",
    "print(labels)\n",
    "\n",
    "\n",
    "def making_epochs(data):\n",
    "    epochs = []\n",
    "    for b in beep_pos:\n",
    "        epochs.append(data[:,b+sampling_rate*1:b+sampling_rate*4])\n",
    "    return np.asarray(epochs)\n",
    "\n",
    "\n",
    "epochs = making_epochs(data)\n",
    "# dif = [int(pos[i]) - int(pos[i-1]) for i in range(len(beep)-1) ]\n",
    "\n",
    "# print(dif[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "\n",
    "epochs_std = copy.copy(epochs)\n",
    "\n",
    "\n",
    "sample_num, chan_num, timepoint = epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    epochs_std[:,c,:] = chan_std\n",
    "    \n",
    "\n",
    "print(np.array_equal(epochs_std, epochs))\n",
    "print(epochs_std.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np\n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(y_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "testing_epochs = []\n",
    "single_epoch = []\n",
    "samples, chan_num, timesteps = epochs.shape\n",
    "epochs_power = epochs**2   \n",
    "# for s in range(samples):\n",
    "#     single_epoch = []\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0])\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,30,:], 'db4', level=4)[0])\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,33,:], 'db4', level=4)[0])\n",
    "#     testing_epochs.append(np.asarray(single_epoch))\n",
    "    \n",
    "    \n",
    "for s in range(samples):\n",
    "    single_epoch = []\n",
    "    single_epoch = np.asarray(single_epoch).reshape((1,-1))\n",
    "    for xi in pywt.wavedec(epochs_power[s,27,:], 'db4', level=4):\n",
    "        single_epoch = np.concatenate((single_epoch,np.asarray(xi).reshape((1,-1))), axis = 1)\n",
    "\n",
    "    testing_epochs.append(single_epoch)\n",
    "  \n",
    "\n",
    "testing_epochs = np.asarray(testing_epochs)\n",
    "print(testing_epochs.shape)\n",
    "#9540\n",
    "\n",
    "print(np.count_nonzero(abs(testing_epochs) > 1.0056865244277509e-9))\n",
    "print(np.min(abs(testing_epochs)))\n",
    "\n",
    "\n",
    "testing_epochs = testing_epochs.reshape((180, 775))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(testing_epochs)\n",
    "# print(scaler.mean_)\n",
    "testing_epochs = scaler.transform(testing_epochs)\n",
    "\n",
    "print(testing_epochs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0], pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[1])\n",
    "\n",
    "y = []\n",
    "y = np.asarray(y).reshape((1,-1))\n",
    "for xi in pywt.wavedec(epochs_power[s,27,:], 'db4', level=4):\n",
    "    y = np.concatenate((y,np.asarray(xi).reshape((1,-1))), axis = 1)\n",
    "    \n",
    "# print(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0].shape)\n",
    "#378 192 99  53  53\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "print(np.asarray(weights)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# testing_epochs = testing_epochs.reshape((180,1, 775))\n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "stop_here_please = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)\n",
    "\n",
    "\n",
    "testing_epochs = np.asarray(testing_epochs)\n",
    "\n",
    "# testing_epochs = testing_epochs.swapaxes(1,2)\n",
    "\n",
    "# time_steps = 53\n",
    "# n_features = 3\n",
    "\n",
    "time_steps = 775\n",
    "n_features = 1\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "testing_epochs_ANN = testing_epochs.reshape(testing_epochs.shape[0], -1)\n",
    "cvscores = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(testing_epochs):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = testing_epochs[train_idx]\n",
    "    X_test = testing_epochs[test_idx]\n",
    "    \n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 5, input_dim=775, activation=\"relu\", \\\n",
    "                    kernel_initializer=\"uniform\", kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 5, init='uniform', activation='sigmoid'))\n",
    "    model.compile( loss = \"categorical_crossentropy\", \n",
    "#                optimizer = keras.optimizers.SGD(lr=0.01, clipvalue=0.5), \n",
    "                optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0),\n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "    \n",
    "  #  batch_size cannot be 1, at least contain all the classes for cross-entropy\n",
    "    model.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size=18, epochs=100, callbacks=[stop_here_please])\n",
    "\n",
    "    print(model.predict_classes(X_train))\n",
    "    print(y_train)\n",
    "\n",
    "\n",
    "#     print(history.losses)\n",
    "\n",
    "    \n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(3, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(LSTM(10, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "#     model.add(Dense(50, activation='sigmoid'))\n",
    "#     model.add(Dense(5, activation='sigmoid'))\n",
    "#     model.compile(loss=\"categorical_crossentropy\",\n",
    "#                   optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False), \n",
    "#                   metrics=['accuracy'])\n",
    "#     model.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size=36, epochs=2000, callbacks=[history])\n",
    "\n",
    "\n",
    "\n",
    "    scores = model.evaluate(X_test, keras.utils.np_utils.to_categorical(y_test), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(np.mean(cvscores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_classes(X_test))\n",
    "print(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(scaler.mean_)\n",
    "print(scaler.transform(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline PCA + Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "\n",
    "samples = []\n",
    "for s in range(epochs_std.shape[0]):\n",
    "    X = epochs_std[s,:,:]\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    pca_com = pca.components_.flatten()\n",
    "    samples.append(pca_com)\n",
    "samples = np.asarray(samples)\n",
    "# print(samples.shape)\n",
    "# logistic = linear_model.LogisticRegression()\n",
    "# logistic.fit(samples, np.asarray(labels))\n",
    "# y_pred = logistic.predict(samples)\n",
    "\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(samples):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = samples[train_idx]\n",
    "    X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    y_predict = logistic.predict(X_test)\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline PCA + NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "\n",
    "samples = []\n",
    "for s in range(epochs_std.shape[0]):\n",
    "    X = epochs_std[s,:,:]\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    pca_com = pca.components_.flatten()\n",
    "    samples.append(pca_com)\n",
    "samples = np.asarray(samples)\n",
    "print(samples.shape)\n",
    "\n",
    "\n",
    "\n",
    "cvscores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(samples):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = samples[train_idx]\n",
    "    X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 5, input_dim=1500, activation=\"relu\", \\\n",
    "                    kernel_initializer=\"uniform\", kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 5, init='uniform', activation='sigmoid'))\n",
    "    model.compile( loss = \"categorical_crossentropy\", \n",
    "#                optimizer = keras.optimizers.SGD(lr=0.01, clipvalue=0.5), \n",
    "                optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0),\n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "    \n",
    "  #  batch_size cannot be 1, at least contain all the classes for cross-entropy\n",
    "    model.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size=18, epochs=500, callbacks=[stop_here_please])\n",
    "\n",
    "\n",
    "    \n",
    "    scores = model.evaluate(X_test, keras.utils.np_utils.to_categorical(y_test), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(np.mean(cvscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Common spatial patten + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['1' '4' '4' '3' '1' '3' '2' '1' '4' '3' '3' '1' '3' '2' '4' '1' '3' '4'\n",
      " '1' '4' '4' '3' '2' '2' '3' '3' '4' '3' '2' '2' '1' '3' '1' '4' '3' '3']\n",
      "['1' '4' '3' '3' '1' '4' '1' '1' '4' '3' '3' '2' '4' '2' '3' '1' '4' '4'\n",
      " '3' '4' '3' '4' '2' '2' '4' '4' '4' '3' '1' '2' '1' '3' '1' '4' '3' '3']\n",
      "0.6388888888888888\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kun\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['2' '4' '2' '2' '2' '2' '2' '1' '2' '3' '2' '3' '4' '4' '4' '4' '3' '1'\n",
      " '3' '4' '2' '3' '3' '4' '1' '1' '1' '3' '1' '4' '1' '1' '1' '2' '2' '3']\n",
      "['2' '3' '2' '2' '2' '2' '1' '1' '2' '3' '1' '3' '4' '3' '4' '4' '3' '1'\n",
      " '4' '4' '2' '3' '3' '4' '1' '1' '1' '4' '1' '4' '1' '1' '1' '2' '2' '3']\n",
      "0.8333333333333334\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['1' '1' '1' '3' '4' '2' '2' '3' '3' '3' '1' '1' '3' '3' '1' '4' '3' '1'\n",
      " '3' '4' '2' '1' '2' '3' '1' '3' '1' '1' '2' '1' '1' '1' '3' '2' '1' '1']\n",
      "['1' '1' '1' '3' '4' '2' '2' '3' '4' '3' '1' '2' '3' '3' '1' '4' '1' '3'\n",
      " '3' '4' '2' '4' '1' '3' '1' '4' '1' '1' '2' '1' '1' '2' '4' '2' '1' '1']\n",
      "0.75\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['3' '2' '1' '1' '2' '4' '4' '2' '4' '1' '4' '1' '3' '3' '2' '2' '1' '3'\n",
      " '2' '3' '3' '4' '2' '4' '4' '1' '1' '1' '4' '4' '4' '3' '1' '4' '3' '3']\n",
      "['3' '2' '1' '3' '2' '4' '3' '1' '4' '1' '3' '1' '3' '3' '2' '2' '1' '4'\n",
      " '2' '3' '3' '4' '1' '4' '4' '1' '1' '2' '4' '4' '4' '3' '1' '3' '3' '4']\n",
      "0.75\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['1' '1' '3' '2' '4' '1' '3' '4' '2' '3' '1' '1' '3' '3' '4' '2' '1' '2'\n",
      " '3' '2' '2' '1' '2' '2' '1' '1' '4' '2' '2' '1' '2' '3' '4' '4' '2' '2']\n",
      "['1' '2' '3' '2' '4' '3' '3' '4' '2' '3' '1' '2' '3' '3' '4' '2' '2' '2'\n",
      " '3' '2' '1' '3' '1' '2' '1' '1' '3' '2' '2' '1' '2' '3' '4' '4' '2' '1']\n",
      "0.75\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['3' '3' '1' '4' '4' '4' '4' '1' '2' '2' '3' '1' '2' '2' '2' '4' '4' '1'\n",
      " '2' '1' '3' '3' '3' '2' '2' '4' '4' '4' '3' '2' '1' '2' '4' '4' '1' '1']\n",
      "['3' '3' '4' '4' '4' '4' '3' '1' '2' '2' '3' '2' '2' '2' '2' '4' '4' '1'\n",
      " '2' '1' '3' '3' '3' '2' '2' '4' '4' '3' '3' '2' '1' '2' '4' '4' '1' '3']\n",
      "0.8611111111111112\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "['2' '2' '1' '2' '1' '4' '2' '4' '1' '3' '1' '2' '2' '3' '3' '4' '2' '2'\n",
      " '2' '2' '1' '1' '1' '3' '3' '1' '3' '3' '2' '4' '4' '1' '2' '1' '2' '3']\n",
      "['2' '1' '1' '2' '1' '4' '2' '4' '1' '3' '1' '1' '1' '3' '3' '4' '2' '2'\n",
      " '2' '2' '1' '1' '1' '3' '3' '3' '3' '4' '2' '4' '4' '1' '1' '1' '2' '3']\n",
      "0.8333333333333334\n",
      "(144, 61, 750)\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from mne import Epochs, pick_types, find_events\n",
    "from mne.channels import read_layout\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "\n",
    "\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "# cv_split = cv.split(epochs_std)\n",
    "\n",
    "# # Assemble a classifier\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "\n",
    "# # Use scikit-learn Pipeline with cross_val_score function\n",
    "# clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "# scores = cross_val_score(clf, epochs_std, np.asarray(labels), cv=cv, n_jobs=1)\n",
    "\n",
    "# # Printing the results\n",
    "# class_balance = np.mean(labels == labels[0])\n",
    "# class_balance = max(class_balance, 1. - class_balance)\n",
    "# print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "#                                                           class_balance))\n",
    "\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(epochs_std):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_std[train_idx]\n",
    "    X_test = epochs_std[test_idx]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "    new_epochs = csp.fit_transform(X_train,  y_train)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(new_epochs, y_train)\n",
    "    \n",
    "\n",
    "    y_pred_csp = csp.transform(X_test)\n",
    "    y_pred = lda.predict(y_pred_csp)\n",
    "    \n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['2' '1' '3' '1' '1' '2' '4' '2' '3' '2' '3' '3' '2' '2' '3' '4' '2' '2'\n",
    " '3' '1' '3' '3' '1' '3' '2' '4' '2' '3' '1' '4' '4' '3' '3' '2' '4' '3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
