{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from k3b.gdf...\n",
      "GDF file detected\n",
      "Overlapping events detected. Use find_edf_events for the original events.\n",
      "Setting channel info structure...\n",
      "Interpolating stim channel. Events may jitter.\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 986779  =      0.000 ...  3947.116 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-3bd60b140d72>:1: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw = mne.io.read_raw_edf('k3b.gdf', preload=True, stim_channel='auto')\n",
      "<ipython-input-2-3bd60b140d72>:1: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw = mne.io.read_raw_edf('k3b.gdf', preload=True, stim_channel='auto')\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_edf('k3b.gdf', preload=True, stim_channel='auto')\n",
    "\n",
    "# print(raw.ch_names)\n",
    "\n",
    "# events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "# picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                    exclude='bads')\n",
    "# tmin, tmax = 1., 4.\n",
    "\n",
    "\n",
    "# event_id = [ 768, 1791, 2323, 2324, 2325, 2326, 2337, 2339, 3346, 3347, 3348, 3349, 3360, 3362]\n",
    "# epochs = mne.Epochs(raw, events, None, tmin, tmax, proj=True, picks=picks,\n",
    "#                 baseline=None, preload=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 31\n"
     ]
    }
   ],
   "source": [
    "print(raw.ch_names[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 events found\n",
      "Event IDs: [ 768 1791 2323 2324 2325 2326 2337 2339 3346 3347 3348 3349 3360 3362]\n",
      "(61, 986780)\n",
      "2924\n",
      "2924\n",
      "[ 2424  2924  2924  3174  4984  5484  5484  5734  7544  7544  8044  8044\n",
      "  8294 10104 10604 10604 10854 12665 13165 13165]\n",
      "1502\n",
      "1502\n",
      "['3', '4', '3', '3', '3', '2', '4', '2', '1', '4', '1', '2', '2', '1', '2', '4', '1', '2', '3', '1', '1', '3', '1', '3', '1', '1', '1', '2', '4', '3', '2', '4', '2', '2', '3', '4', '1', '3', '2', '4', '3', '1', '4', '1', '4', '3', '1', '1', '2', '3', '1', '4', '4', '2', '2', '3', '4', '2', '1', '4', '4', '3', '1', '3', '4', '3', '3', '3', '1', '4', '2', '4', '4', '2', '1', '1', '4', '1', '3', '2', '4', '2', '1', '4', '2', '2', '4', '3', '1', '2', '2', '4', '2', '3', '4', '4', '3', '3', '4', '1', '4', '2', '3', '2', '1', '3', '4', '1', '2', '1', '3', '1', '3', '4', '3', '3', '1', '4', '3', '4', '4', '2', '2', '1', '2', '4', '4', '3', '3', '1', '4', '1', '3', '1', '2', '2', '3', '3', '3', '1', '1', '2', '2', '2', '1', '1', '2', '2', '1', '1', '4', '2', '4', '3', '4', '3', '3', '1', '1', '2', '2', '1', '3', '3', '3', '4', '4', '2', '3', '1', '1', '2', '4', '2', '4', '2', '4', '4', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "data = raw.get_data()\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "# raw = mne.io.read_raw_edf(\"k3b.gdf\", preload=True)\n",
    "_, pos, kind, chan, dur = raw.find_edf_events()\n",
    "kind_hex = np.array([hex(n) for n in kind])\n",
    "\n",
    "print(pos[1])\n",
    "print(pos[2])\n",
    "print(pos[:20])\n",
    "\n",
    "print(len(kind_hex))\n",
    "print(len(pos))\n",
    "sampling_rate = 250\n",
    "\n",
    "\n",
    "beep_pos = [pos[i] for i in range(len(kind_hex)) if kind_hex[i] in ['0x301','0x302','0x303','0x304']]\n",
    "labels = [i[4] for i in kind_hex if i in ['0x301','0x302','0x303','0x304']]\n",
    "print(labels)\n",
    "\n",
    "\n",
    "def making_epochs(data):\n",
    "    epochs = []\n",
    "    for b in beep_pos:\n",
    "        epochs.append(data[:,b+sampling_rate*1:b+sampling_rate*4])\n",
    "    return np.asarray(epochs)\n",
    "\n",
    "\n",
    "epochs = making_epochs(data)\n",
    "# dif = [int(pos[i]) - int(pos[i-1]) for i in range(len(beep)-1) ]\n",
    "\n",
    "# print(dif[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 61, 750)\n"
     ]
    }
   ],
   "source": [
    "print(epochs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "(144,)\n",
      "1.0\n",
      "Classification accuracy: 0.913889 / Chance level: 0.250000\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import numpy as np\n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(y_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 1, 775)\n",
      "91\n",
      "1.0490812995593966e-16\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "testing_epochs = []\n",
    "single_epoch = []\n",
    "samples, chan_num, timesteps = epochs.shape\n",
    "epochs_power = epochs**2   \n",
    "# for s in range(samples):\n",
    "#     single_epoch = []\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0])\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,30,:], 'db4', level=4)[0])\n",
    "#     single_epoch.append(pywt.wavedec(epochs_power[s,33,:], 'db4', level=4)[0])\n",
    "#     testing_epochs.append(np.asarray(single_epoch))\n",
    "    \n",
    "    \n",
    "for s in range(samples):\n",
    "    single_epoch = []\n",
    "    single_epoch = np.asarray(single_epoch).reshape((1,-1))\n",
    "    for xi in pywt.wavedec(epochs_power[s,27,:], 'db4', level=4):\n",
    "        single_epoch = np.concatenate((single_epoch,np.asarray(xi).reshape((1,-1))), axis = 1)\n",
    "\n",
    "    testing_epochs.append(single_epoch)\n",
    "  \n",
    "\n",
    "testing_epochs = np.asarray(testing_epochs)\n",
    "print(testing_epochs.shape)\n",
    "#9540\n",
    "\n",
    "print(np.count_nonzero(abs(testing_epochs) > 1.0056865244277509e-9))\n",
    "print(np.min(abs(testing_epochs)))\n",
    "\n",
    "\n",
    "testing_epochs = testing_epochs.reshape((180, 775))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(testing_epochs)\n",
    "# print(scaler.mean_)\n",
    "testing_epochs = scaler.transform(testing_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 775)\n"
     ]
    }
   ],
   "source": [
    "# np.concatenate(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0], pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[1])\n",
    "\n",
    "y = []\n",
    "y = np.asarray(y).reshape((1,-1))\n",
    "for xi in pywt.wavedec(epochs_power[s,27,:], 'db4', level=4):\n",
    "    y = np.concatenate((y,np.asarray(xi).reshape((1,-1))), axis = 1)\n",
    "    \n",
    "# print(pywt.wavedec(epochs_power[s,27,:], 'db4', level=4)[0].shape)\n",
    "#378 192 99  53  53\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02026197  0.01008344  0.0281049  -0.00588483 -0.02329246 -0.01131399\n",
      "  0.02286122  0.02807407 -0.02480852  0.02144835 -0.03157116  0.02203869\n",
      "  0.02432689 -0.03360921 -0.01371121  0.0332848   0.02939171 -0.01283365\n",
      "  0.00913258  0.03174998 -0.01887629  0.02043019  0.03358698 -0.02914592\n",
      " -0.01815564  0.01274412  0.02527639  0.0003182   0.01464843  0.00697661\n",
      "  0.0038463   0.02804249  0.0028288  -0.00961178  0.03710476  0.01277776\n",
      " -0.01071792  0.00165564  0.02713612 -0.01148787 -0.00614739 -0.00524904\n",
      "  0.01256036  0.01731174  0.01820919  0.02143856 -0.00090359 -0.00530857\n",
      "  0.03376178 -0.01795817 -0.00377397  0.00915935 -0.01166671  0.00854721\n",
      " -0.00649501  0.02719112 -0.00645062  0.0202097   0.00469979  0.02829024\n",
      "  0.00837883 -0.02156244 -0.01468641 -0.02569531 -0.01841003 -0.00140771\n",
      " -0.03338108 -0.0058276   0.02066448  0.02221901 -0.00117241 -0.00197715\n",
      "  0.03047566  0.01854157  0.03394988  0.00581654 -0.02598906 -0.00897026\n",
      "  0.02640313 -0.02844475  0.0132376   0.03251541  0.00027956 -0.00661146\n",
      "  0.0115854  -0.01277244 -0.00703739  0.03318005 -0.02689417  0.03141034\n",
      "  0.02611629  0.02594085  0.00939781  0.0321315   0.00871496 -0.01742469\n",
      "  0.0331524   0.03036429  0.01300533  0.01920559 -0.03418653 -0.02233562\n",
      "  0.02764395  0.00015309  0.0274316   0.01409532 -0.01886495  0.0113882\n",
      "  0.01456364 -0.00321789 -0.00744602  0.03233216  0.0088912  -0.01393194\n",
      "  0.0287843  -0.001562   -0.01031269 -0.00440963  0.0309448   0.00371326\n",
      " -0.02374477  0.02270556 -0.02206393  0.00427321 -0.01832296  0.02943397\n",
      "  0.00802294 -0.01903315 -0.00562507  0.02318152  0.01364186 -0.03231984\n",
      " -0.02143947 -0.00571496  0.00563557  0.02577581 -0.02369909  0.02417238\n",
      "  0.00695001 -0.0222999  -0.02696537 -0.00513107 -0.02528641 -0.02462583\n",
      " -0.00499475 -0.01527859  0.03002716 -0.03014883 -0.0140548   0.0258942\n",
      " -0.00346984 -0.03225381 -0.00520538 -0.01679141  0.02046098 -0.01308537\n",
      " -0.01092848  0.00658603  0.0050655   0.02417193 -0.03154808  0.01219303\n",
      "  0.00377375  0.0056976   0.03323238 -0.02259487 -0.03327915 -0.00154807\n",
      " -0.01209743  0.02090404 -0.02861675  0.03009776  0.03120861 -0.01911383\n",
      "  0.0035581  -0.01423463 -0.00296422  0.01453868 -0.02137424  0.02860627\n",
      "  0.01105429 -0.00928766 -0.01215078  0.01098114 -0.00659461  0.00739195\n",
      "  0.01537106  0.03095685  0.002287    0.03520321 -0.01307698  0.00494263\n",
      " -0.03150475  0.0363229   0.0282258   0.02475085  0.00957607 -0.02360586\n",
      " -0.03408802 -0.01084292 -0.01989971 -0.0194961   0.02192946 -0.02459872\n",
      " -0.00045389  0.00510892  0.01636122 -0.00809351  0.03227214 -0.01348608\n",
      " -0.02692022 -0.03208125  0.03224049 -0.00089861 -0.00407307  0.01989812\n",
      " -0.0112799   0.01036339 -0.0242886  -0.0140279  -0.01731992 -0.00470636\n",
      " -0.03050908  0.01852429  0.00868984 -0.03032504  0.02404732  0.00087833\n",
      " -0.01620081  0.0345748   0.01480261 -0.02587726 -0.03096397  0.00366544\n",
      "  0.0343086  -0.01729473  0.00733683  0.03622911 -0.03008253  0.00695174\n",
      " -0.01477839 -0.02645742 -0.03188766  0.00601602 -0.0028816  -0.00511229\n",
      " -0.03410487 -0.00347933  0.02926873 -0.03378476  0.00288795  0.02423131\n",
      "  0.02535206  0.00098212  0.00346559  0.00394804  0.01942779  0.01490976\n",
      "  0.02183802 -0.00089563 -0.02452554 -0.00836484  0.01428244  0.01386398\n",
      " -0.02626656  0.03054395  0.01337024 -0.01643564  0.00400052 -0.00176969\n",
      " -0.01750002  0.0156128  -0.02538651 -0.03443186 -0.01923392 -0.01446818\n",
      "  0.03187933  0.01191043  0.02238855  0.00480783  0.01146988 -0.00334038\n",
      " -0.00945177 -0.03271879  0.00206495  0.02327632  0.02714636 -0.01024643\n",
      "  0.03504323 -0.02966133 -0.01805601  0.01924871  0.02290646 -0.01474812\n",
      " -0.0341781   0.00961154 -0.00871914 -0.0293609  -0.00852231 -0.00010311\n",
      " -0.00209801 -0.03097872  0.02655049 -0.02307866  0.02722093 -0.03518895\n",
      " -0.01184076 -0.02696507  0.01213351 -0.01298257  0.02394569  0.01051865\n",
      "  0.02981101  0.0035707  -0.0221913   0.01873935 -0.01970331  0.03479731\n",
      "  0.00405608 -0.03452194  0.03465723  0.02002228  0.02397351  0.01803391\n",
      " -0.00265277 -0.01226943  0.00630998  0.03482934  0.00238832  0.0217357\n",
      " -0.00576459  0.00766413 -0.01963132  0.02197086 -0.00559916 -0.00510194\n",
      " -0.00419681  0.01439012  0.00192406  0.00629731  0.00685939  0.03106497\n",
      " -0.0210619   0.02610903 -0.03168907 -0.01156336 -0.03646146 -0.01395368\n",
      "  0.00771781 -0.03340857 -0.03643937 -0.01396245 -0.0077193   0.02779318\n",
      " -0.02829701  0.03224343 -0.01159135 -0.0342419   0.01685449  0.0252712\n",
      "  0.03473554 -0.00258154  0.00987001  0.02898809 -0.00927889  0.00382751\n",
      " -0.00087168 -0.0001746  -0.0073114  -0.0053438  -0.0031475   0.01876188\n",
      " -0.00224148 -0.02575509  0.02306568 -0.00588624 -0.00833619 -0.01760902\n",
      " -0.0196306   0.02669751  0.02759459 -0.02043233 -0.03480289 -0.02533966\n",
      "  0.02494824 -0.00951515 -0.00123182 -0.01372964 -0.00419784  0.02711766\n",
      " -0.01098161  0.0154253  -0.00990764  0.0253962  -0.01064866  0.0066137\n",
      " -0.02506381  0.0079743   0.02388573  0.01743854 -0.01093017 -0.00186479\n",
      "  0.01219819  0.01488443 -0.01895618 -0.0106828  -0.00117968 -0.0003653\n",
      "  0.02485644  0.01140012 -0.01365281 -0.00235866  0.01352335  0.03304929\n",
      " -0.02964226  0.03427568  0.01911632  0.01459373  0.01787161  0.03368654\n",
      "  0.00111599  0.00634038 -0.02391038  0.01177487 -0.02724277  0.03078987\n",
      "  0.01706193  0.00711115  0.01205228  0.00887712 -0.03298102 -0.00795453\n",
      "  0.02254541 -0.02502375 -0.01400305  0.02771822  0.03175854 -0.00479338\n",
      "  0.00660596  0.03111236  0.02975217 -0.00988776 -0.02403589  0.02044705\n",
      " -0.01481702  0.02203857 -0.02196003  0.03056293  0.02059285 -0.02229377\n",
      " -0.0193025   0.02465127  0.02162832 -0.01559731 -0.00982711  0.0332517\n",
      "  0.01417452  0.01673906  0.01574071 -0.02991295  0.02997392  0.02932325\n",
      " -0.00292633 -0.03535675  0.01893369 -0.01441726 -0.00887354  0.0352264\n",
      " -0.02206881 -0.00301467  0.03165083  0.006495    0.03255217  0.01133606\n",
      " -0.00839876  0.02156499 -0.03261995  0.03205229 -0.00255482 -0.02782326\n",
      " -0.03288589  0.03200491  0.01198068  0.03218432  0.00041275 -0.03202946\n",
      " -0.03189175 -0.02502443 -0.02095839 -0.01105637 -0.0116077  -0.02199282\n",
      "  0.00225307  0.00407811 -0.00060069 -0.0180732   0.0155389   0.01668917\n",
      "  0.0315186   0.00346657  0.02121724  0.02156636 -0.02522247  0.0079779\n",
      "  0.0171247  -0.01774927 -0.01387812 -0.00522123  0.03290888  0.00438613\n",
      " -0.00038613 -0.03117051  0.03286916 -0.00369035 -0.00919363 -0.01950546\n",
      " -0.02232694  0.01488714  0.03326755  0.01264646 -0.0193641  -0.02019133\n",
      "  0.00678263 -0.01163009  0.01021474  0.01451721 -0.00670826 -0.02585527\n",
      "  0.01541193 -0.01211629 -0.01374481 -0.01406226 -0.00087652 -0.00856759\n",
      "  0.01030033  0.00681636  0.01211966 -0.02503126  0.01817621  0.0299048\n",
      "  0.0076312   0.02946177 -0.02623332 -0.01827509 -0.03053131 -0.00158517\n",
      "  0.0040113   0.0249291  -0.0096358   0.0124754  -0.00255443 -0.0234192\n",
      " -0.01540705  0.01552311  0.03338603 -0.02898925 -0.01505733 -0.00994785\n",
      "  0.00025627  0.01350761 -0.03503631 -0.01562302 -0.0333843   0.02960233\n",
      " -0.01220053  0.0144542  -0.00887309 -0.03268263 -0.03438728 -0.00609939\n",
      "  0.00027018 -0.02022758 -0.01122961  0.01973009 -0.01574129 -0.01229696\n",
      " -0.02413375 -0.03543258  0.0187978  -0.02244826  0.03193931  0.01099025\n",
      " -0.02995714  0.00536152 -0.01559966 -0.01167576 -0.02603899  0.03372999\n",
      " -0.00071728 -0.03052091  0.0255278   0.02307438 -0.02805803 -0.00529929\n",
      "  0.0063655   0.00655109 -0.00260904  0.01120086  0.01896516  0.01663441\n",
      " -0.00413152 -0.01370017  0.02638935 -0.01085512 -0.02754326 -0.00522614\n",
      " -0.02699047  0.01561687 -0.03189231 -0.0262249  -0.03029472  0.02878542\n",
      " -0.02005883  0.02813278 -0.01996856 -0.03502631  0.02555241 -0.00949298\n",
      " -0.0341783   0.02569218 -0.03096693 -0.02235109 -0.03512504 -0.01431922\n",
      "  0.01491749 -0.0200226   0.01304625  0.02299099 -0.02293589  0.02021145\n",
      " -0.01249253  0.00194796  0.0198807   0.02549561 -0.03012471 -0.03095805\n",
      "  0.02325442 -0.00716844 -0.01414534  0.00175003  0.01765373 -0.03464636\n",
      " -0.02620871 -0.03612303  0.03519124 -0.01629618  0.02047542 -0.02306282\n",
      " -0.01648568 -0.00681359 -0.02363764 -0.03503394 -0.02929773  0.02436183\n",
      " -0.00360404 -0.00748756  0.01880787  0.02716641  0.00155052  0.01054151\n",
      "  0.00345404  0.0290898  -0.02518672  0.01136697 -0.01501133  0.00191763\n",
      "  0.01898636  0.00066018  0.02320267  0.00520002 -0.00116842 -0.03070011\n",
      " -0.00164522 -0.00030657 -0.0084565   0.03201297 -0.02303322 -0.01265803\n",
      " -0.00189437  0.03030601 -0.02243932 -0.03514928  0.02857211 -0.02613446\n",
      " -0.03239836 -0.01395831  0.03648194  0.01684424 -0.00833629 -0.02158869\n",
      " -0.0053563   0.01662779 -0.00189182  0.0320409  -0.02202201  0.00173325\n",
      " -0.01815153 -0.00938973 -0.02359338 -0.0190035  -0.03442244  0.02807476\n",
      "  0.01288053 -0.03238068  0.00637818  0.01626167 -0.0039726  -0.02628418\n",
      "  0.02264684  0.02736416 -0.02862123  0.03449031  0.0314987  -0.01849275\n",
      " -0.02233281 -0.02748542 -0.01018458 -0.01923803 -0.02068023  0.0139359\n",
      " -0.00108442  0.00113644 -0.00250463  0.02452878 -0.03366673 -0.02075171\n",
      " -0.00640582  0.01486074 -0.00827866 -0.0155656   0.03518366  0.02962809\n",
      " -0.0154758  -0.02955146  0.01117563  0.03299332  0.02168654 -0.03423869\n",
      "  0.00609837 -0.00056811  0.01193682 -0.02216271  0.00401073 -0.02685281\n",
      "  0.02763105  0.02870343 -0.03631581 -0.02849923 -0.01140255 -0.00177078\n",
      " -0.00506613 -0.02670388 -0.02824947  0.03208422 -0.02787519 -0.02475285\n",
      "  0.01280454 -0.03137118 -0.00130542 -0.03224939 -0.03151134  0.01849739\n",
      " -0.02147081 -0.0067815   0.0030126   0.02383782  0.01428197 -0.00779791\n",
      "  0.02435685 -0.03649372  0.03606583 -0.02839085  0.00645068 -0.00405202\n",
      "  0.02148522]\n"
     ]
    }
   ],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "print(np.asarray(weights)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kun\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "144/144 [==============================] - 0s 968us/step - loss: 1.8218 - acc: 0.2292\n",
      "Epoch 2/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.7073 - acc: 0.2361\n",
      "Epoch 3/100\n",
      "144/144 [==============================] - 0s 53us/step - loss: 1.6562 - acc: 0.3611\n",
      "Epoch 4/100\n",
      "144/144 [==============================] - 0s 60us/step - loss: 1.6274 - acc: 0.4236\n",
      "Epoch 5/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.6100 - acc: 0.5208\n",
      "Epoch 6/100\n",
      "144/144 [==============================] - 0s 99us/step - loss: 1.6005 - acc: 0.5208\n",
      "Epoch 7/100\n",
      "144/144 [==============================] - 0s 111us/step - loss: 1.5916 - acc: 0.5208\n",
      "Epoch 8/100\n",
      "144/144 [==============================] - 0s 109us/step - loss: 1.5850 - acc: 0.5347\n",
      "Epoch 9/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.5744 - acc: 0.6181\n",
      "Epoch 10/100\n",
      "144/144 [==============================] - 0s 111us/step - loss: 1.5644 - acc: 0.6319\n",
      "Epoch 11/100\n",
      "144/144 [==============================] - 0s 84us/step - loss: 1.5541 - acc: 0.5903\n",
      "Epoch 12/100\n",
      "144/144 [==============================] - 0s 65us/step - loss: 1.5472 - acc: 0.5972\n",
      "Epoch 13/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.5397 - acc: 0.6042\n",
      "Epoch 14/100\n",
      "144/144 [==============================] - 0s 63us/step - loss: 1.5309 - acc: 0.5903\n",
      "Epoch 15/100\n",
      " 18/144 [==>...........................] - ETA: 0s - loss: 1.4890 - acc: 0.6667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kun\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\callbacks.py:535: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 0s 56us/step - loss: 1.5066 - acc: 0.6250\n",
      "Epoch 16/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.4936 - acc: 0.5556\n",
      "Epoch 17/100\n",
      "144/144 [==============================] - 0s 106us/step - loss: 1.4839 - acc: 0.6181\n",
      "Epoch 18/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.4691 - acc: 0.6250\n",
      "Epoch 19/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.4681 - acc: 0.6042\n",
      "Epoch 20/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.4618 - acc: 0.5556\n",
      "Epoch 21/100\n",
      "144/144 [==============================] - 0s 65us/step - loss: 1.4351 - acc: 0.6250\n",
      "Epoch 22/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.4332 - acc: 0.5903\n",
      "Epoch 23/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.4075 - acc: 0.6250\n",
      "Epoch 24/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.4043 - acc: 0.5903\n",
      "Epoch 25/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.3839 - acc: 0.6111\n",
      "Epoch 26/100\n",
      "144/144 [==============================] - 0s 99us/step - loss: 1.3590 - acc: 0.5972\n",
      "Epoch 27/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.3596 - acc: 0.5903\n",
      "Epoch 28/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.3434 - acc: 0.6111\n",
      "Epoch 29/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.3405 - acc: 0.5625\n",
      "Epoch 30/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.3386 - acc: 0.5764\n",
      "Epoch 31/100\n",
      "144/144 [==============================] - 0s 62us/step - loss: 1.2906 - acc: 0.6667\n",
      "Epoch 32/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.2993 - acc: 0.5694\n",
      "Epoch 33/100\n",
      "144/144 [==============================] - 0s 55us/step - loss: 1.2652 - acc: 0.6111\n",
      "Epoch 34/100\n",
      "144/144 [==============================] - 0s 92us/step - loss: 1.2465 - acc: 0.6875\n",
      "Epoch 35/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.2858 - acc: 0.6181\n",
      "Epoch 36/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.2650 - acc: 0.6042\n",
      "Epoch 37/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.2363 - acc: 0.6667\n",
      "Epoch 38/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.2399 - acc: 0.6458\n",
      "Epoch 39/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.2175 - acc: 0.6875\n",
      "Epoch 40/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.2046 - acc: 0.7222\n",
      "Epoch 41/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.1967 - acc: 0.7500\n",
      "Epoch 42/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.1900 - acc: 0.7292\n",
      "Epoch 43/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.1951 - acc: 0.7500\n",
      "Epoch 44/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.1735 - acc: 0.7222\n",
      "Epoch 45/100\n",
      "144/144 [==============================] - 0s 70us/step - loss: 1.1744 - acc: 0.6875\n",
      "Epoch 46/100\n",
      "144/144 [==============================] - 0s 135us/step - loss: 1.1696 - acc: 0.6528\n",
      "Epoch 47/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.1569 - acc: 0.6667\n",
      "Epoch 48/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.1260 - acc: 0.7569\n",
      "Epoch 49/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.1559 - acc: 0.6736\n",
      "Epoch 50/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.1393 - acc: 0.6667\n",
      "Epoch 51/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.1025 - acc: 0.7083\n",
      "Epoch 52/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0951 - acc: 0.7153\n",
      "Epoch 53/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0731 - acc: 0.7222\n",
      "Epoch 54/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0680 - acc: 0.6944\n",
      "Epoch 55/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.1150 - acc: 0.6875\n",
      "Epoch 56/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.0634 - acc: 0.7083\n",
      "Epoch 57/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 1.0353 - acc: 0.7500\n",
      "Epoch 58/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0507 - acc: 0.6597\n",
      "Epoch 59/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0476 - acc: 0.6806\n",
      "Epoch 60/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0554 - acc: 0.7569\n",
      "Epoch 61/100\n",
      "144/144 [==============================] - 0s 111us/step - loss: 1.0219 - acc: 0.7431\n",
      "Epoch 62/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9727 - acc: 0.7569\n",
      "Epoch 63/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.0711 - acc: 0.6667\n",
      "Epoch 64/100\n",
      "144/144 [==============================] - 0s 55us/step - loss: 1.0108 - acc: 0.7431\n",
      "Epoch 65/100\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.9635 - acc: 0.7708\n",
      "Epoch 66/100\n",
      "144/144 [==============================] - 0s 99us/step - loss: 1.0221 - acc: 0.7014\n",
      "Epoch 67/100\n",
      "144/144 [==============================] - 0s 40us/step - loss: 1.0210 - acc: 0.7014\n",
      "Epoch 68/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9679 - acc: 0.7639\n",
      "Epoch 69/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9515 - acc: 0.7361\n",
      "Epoch 70/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9533 - acc: 0.7222\n",
      "Epoch 71/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9680 - acc: 0.6944\n",
      "Epoch 72/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9961 - acc: 0.7222\n",
      "Epoch 73/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9332 - acc: 0.7639\n",
      "Epoch 74/100\n",
      "144/144 [==============================] - 0s 28us/step - loss: 0.9869 - acc: 0.7083\n",
      "Epoch 75/100\n",
      "144/144 [==============================] - 0s 46us/step - loss: 0.9363 - acc: 0.7431\n",
      "Epoch 76/100\n",
      "144/144 [==============================] - 0s 68us/step - loss: 0.9681 - acc: 0.7569\n",
      "Epoch 77/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.9201 - acc: 0.7708\n",
      "Epoch 78/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.9443 - acc: 0.7292\n",
      "Epoch 79/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9191 - acc: 0.7153\n",
      "Epoch 80/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8937 - acc: 0.7847\n",
      "Epoch 81/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9006 - acc: 0.7778\n",
      "Epoch 82/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9045 - acc: 0.7222\n",
      "Epoch 83/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8746 - acc: 0.7500\n",
      "Epoch 84/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9122 - acc: 0.7292\n",
      "Epoch 85/100\n",
      "144/144 [==============================] - 0s 65us/step - loss: 0.8850 - acc: 0.7431\n",
      "Epoch 86/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8841 - acc: 0.7778\n",
      "Epoch 87/100\n",
      "144/144 [==============================] - 0s 61us/step - loss: 0.8390 - acc: 0.7708\n",
      "Epoch 88/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8524 - acc: 0.7847\n",
      "Epoch 89/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8698 - acc: 0.7500\n",
      "Epoch 90/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8504 - acc: 0.7569\n",
      "Epoch 91/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8230 - acc: 0.7986\n",
      "Epoch 92/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8213 - acc: 0.7778\n",
      "Epoch 93/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8622 - acc: 0.7292\n",
      "Epoch 94/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8379 - acc: 0.7639\n",
      "Epoch 95/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.8582 - acc: 0.7500\n",
      "Epoch 96/100\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.8420 - acc: 0.7708\n",
      "Epoch 97/100\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.9293 - acc: 0.7014\n",
      "Epoch 98/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8228 - acc: 0.7500\n",
      "Epoch 99/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8014 - acc: 0.7847\n",
      "Epoch 100/100\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8247 - acc: 0.7639\n",
      "[4 3 3 4 2 1 2 4 1 3 2 2 1 3 2 3 3 4 1 2 4 1 2 1 3 4 1 4 1 3 1 1 2 2 2 4 2\n",
      " 3 3 1 2 3 4 1 2 1 3 3 2 4 4 1 4 3 3 1 1 3 1 2 2 2 3 2 4 3 2 4 1 4 3 3 3 2\n",
      " 2 1 2 2 1 2 4 2 3 2 2 3 3 2 3 4 4 1 3 2 3 4 3 1 3 2 1 1 3 4 1 2 3 2 2 4 4\n",
      " 3 2 1 1 3 1 2 2 3 1 4 1 3 1 1 3 3 1 4 2 2 1 1 3 1 2 3 1 4 3 2 2 3]\n",
      "['4' '3' '3' '4' '2' '1' '2' '4' '1' '3' '2' '2' '1' '3' '2' '3' '3' '4'\n",
      " '1' '2' '4' '1' '2' '1' '3' '4' '1' '4' '1' '3' '1' '1' '1' '2' '2' '4'\n",
      " '2' '3' '3' '2' '2' '3' '4' '1' '3' '1' '3' '3' '2' '4' '4' '1' '4' '3'\n",
      " '3' '1' '1' '3' '1' '2' '2' '2' '4' '2' '4' '3' '2' '4' '1' '4' '3' '3'\n",
      " '3' '2' '2' '1' '2' '2' '1' '2' '4' '2' '3' '2' '2' '3' '3' '2' '3' '4'\n",
      " '4' '1' '3' '2' '3' '4' '4' '1' '3' '2' '1' '1' '4' '4' '1' '2' '4' '2'\n",
      " '2' '4' '4' '3' '2' '1' '1' '4' '1' '2' '1' '3' '2' '4' '1' '3' '1' '1'\n",
      " '4' '4' '1' '4' '2' '2' '1' '1' '3' '1' '2' '3' '1' '4' '4' '2' '2' '3']\n",
      "acc: 36.11%\n",
      "36.11111111111111\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# testing_epochs = testing_epochs.reshape((180,1, 775))\n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "stop_here_please = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)\n",
    "\n",
    "\n",
    "testing_epochs = np.asarray(testing_epochs)\n",
    "\n",
    "# testing_epochs = testing_epochs.swapaxes(1,2)\n",
    "\n",
    "# time_steps = 53\n",
    "# n_features = 3\n",
    "\n",
    "time_steps = 775\n",
    "n_features = 1\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "testing_epochs_ANN = testing_epochs.reshape(testing_epochs.shape[0], -1)\n",
    "cvscores = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(testing_epochs):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = testing_epochs[train_idx]\n",
    "    X_test = testing_epochs[test_idx]\n",
    "    \n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 5, input_dim=775, activation=\"relu\", \\\n",
    "                    kernel_initializer=\"uniform\", kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 5, init='uniform', activation='sigmoid'))\n",
    "    model.compile( loss = \"categorical_crossentropy\", \n",
    "#                optimizer = keras.optimizers.SGD(lr=0.01, clipvalue=0.5), \n",
    "                optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0),\n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "    \n",
    "  #  batch_size cannot be 1, at least contain all the classes for cross-entropy\n",
    "    model.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size=18, epochs=100, callbacks=[stop_here_please])\n",
    "\n",
    "    print(model.predict_classes(X_train))\n",
    "    print(y_train)\n",
    "\n",
    "\n",
    "#     print(history.losses)\n",
    "\n",
    "    \n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(3, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(LSTM(10, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "#     model.add(Dense(50, activation='sigmoid'))\n",
    "#     model.add(Dense(5, activation='sigmoid'))\n",
    "#     model.compile(loss=\"categorical_crossentropy\",\n",
    "#                   optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False), \n",
    "#                   metrics=['accuracy'])\n",
    "#     model.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size=36, epochs=2000, callbacks=[history])\n",
    "\n",
    "\n",
    "\n",
    "    scores = model.evaluate(X_test, keras.utils.np_utils.to_categorical(y_test), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(np.mean(cvscores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 4 3 2 2 2 1 3 4 1 4 2 4 2 4 2 2 2 1 4 2 1 4 1 4 2 2 2 1 3 4 4 3 3]\n",
      "['1' '4' '3' '3' '1' '4' '1' '1' '4' '3' '3' '2' '4' '2' '3' '1' '4' '4'\n",
      " '3' '4' '3' '4' '2' '2' '4' '4' '4' '3' '1' '2' '1' '3' '1' '4' '3' '3']\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(X_test))\n",
    "print(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(scaler.mean_)\n",
    "print(scaler.transform(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline PCA + Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 61, 750)\n",
      "Classification accuracy: 0.166667 / Chance level: 0.250000\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "\n",
    "samples = []\n",
    "for s in range(epochs.shape[0]):\n",
    "    X = epochs[s,:,:]\n",
    "    pca = PCA(n_components=20)\n",
    "    pca.fit(X)\n",
    "    pca_com = pca.components_.flatten()\n",
    "    samples.append(pca_com)\n",
    "samples = np.asarray(samples)\n",
    "# print(samples.shape)\n",
    "# logistic = linear_model.LogisticRegression()\n",
    "# logistic.fit(samples, np.asarray(labels))\n",
    "# y_pred = logistic.predict(samples)\n",
    "\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(samples):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = samples[train_idx]\n",
    "    X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    y_predict = logistic.predict(X_test)\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
