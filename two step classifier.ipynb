{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and concatenante data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "# sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "#                'sparse_ANM210861_20130703.npy']\n",
    "\n",
    "# label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "#               'ANM210861_20130703_labels.npy']\n",
    "\n",
    "sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "               'sparse_ANM210861_20130703.npy', 'sparse_ANM210862_20130626.npy', 'sparse_ANM210862_20130627.npy', \n",
    "               'sparse_ANM210862_20130628.npy']\n",
    "\n",
    "label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "              'ANM210861_20130703_labels.npy', 'ANM210862_20130626_labels.npy', 'ANM210862_20130627_labels.npy',\n",
    "              'ANM210862_20130628_labels.npy']\n",
    "\n",
    "\n",
    "agg_spares_epochs = np.load(sparse_files[0])\n",
    "agg_labels = np.load(label_files[0])\n",
    "ind = np.linspace(0,3443-1, num = 1033)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "agg_spares_epochs = agg_spares_epochs[:,:, ind]\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    new_epochs =  np.load(sparse_files[i])\n",
    "    agg_spares_epochs = np.concatenate((agg_spares_epochs, new_epochs), axis = 0)\n",
    "    new_label = np.load(label_files[i])\n",
    "    agg_labels = np.concatenate((agg_labels, new_label), axis = 0)\n",
    "\n",
    "print(agg_spares_epochs.shape)\n",
    "print(agg_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "agg_epochs_std = copy.copy(agg_spares_epochs)\n",
    "sample_num, chan_num, timepoint = agg_spares_epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = agg_spares_epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    agg_epochs_std[:,c,:] = chan_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down sampling with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "ind = np.linspace(0,1032, num = 1032)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "epochs_ds = agg_epochs_std[:,:,ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_ds\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    ts = TangentSpace()\n",
    "    T = ts.fit_transform(cov_X_train, y_train)\n",
    "    \n",
    "#     clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf=SVC(kernel='poly', degree = 2.7,  random_state=0, coef0 =0.2, gamma= 0.03, C=100)\n",
    "    clf.fit(T, y_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    T_test = ts.fit_transform(cov_X_test, y_test)\n",
    "    \n",
    "    y_predict = clf.predict(T_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "ind = np.linspace(0,1032, num = 900)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "epochs_ds = agg_epochs_std[:,:,ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_ds\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    ts = TangentSpace()\n",
    "    T = ts.fit_transform(cov_X_train, y_train)\n",
    "    print(T.shape)\n",
    "    \n",
    "    pca = PCA(n_components=200)\n",
    "    T_pca = pca.fit_transform(T)\n",
    "    print(T_pca.shape)\n",
    "\n",
    "    \n",
    "    clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf.fit(T_pca, y_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    T_test = ts.fit_transform(cov_X_test, y_test)\n",
    "    print(T_test.shape)\n",
    "\n",
    "    pca = PCA(n_components=200)\n",
    "    T_test_pca = pca.fit_transform(T_test)\n",
    "    \n",
    "    \n",
    "    y_predict = clf.predict(T_test_pca)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent Space classifier with single SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "\n",
    "# degree = [1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6]\n",
    "# degree = [2.7, 2.8, 2.9, 3.0, 3.1, 3.2,3.3, 3.4, 3.5, 3.6]\n",
    "degree = np.linspace(3.7, 5.5, 19)\n",
    "coefs = np.linspace(0.0, 0.5, 51)\n",
    "\n",
    "\n",
    "for coef in coefs:\n",
    "    print(coef)\n",
    "    for train_idx, test_idx in cv.split(epochs_data):\n",
    "        start = timeit.default_timer()\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = epochs_data[train_idx]\n",
    "        X_test = epochs_data[test_idx]\n",
    "\n",
    "\n",
    "        cov =  pyriemann.estimation.Covariances('lwf')\n",
    "        cov_X_train = cov.transform(X_train)\n",
    "        cov_X_test = cov.transform(X_test)\n",
    "\n",
    "    #     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=sklearn.discriminant_analysis.LinearDiscriminantAnalysis())\n",
    "    #     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100))\n",
    "        TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='poly', degree = 2.7,  random_state=0, coef0 =coef, gamma= 0.03, C=100))\n",
    "\n",
    "    #     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=LogisticRegression())\n",
    "\n",
    "        TSclassifier.fit(cov_X_train, y_train)\n",
    "\n",
    "\n",
    "        y_predict = TSclassifier.predict(cov_X_test)\n",
    "        print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "        stop = timeit.default_timer()\n",
    "        print('Time: ', stop - start)  \n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))\n",
    "\n",
    "\n",
    "\n",
    "#non-even number of degree\n",
    "# degree =2  70%\n",
    "# degree = 2.2     0.776347\n",
    "#2.6     0.776347 ON AVERAGE\n",
    "#    2.2 with 0.2 coef   => 0.786228!!!    with 0.3 coef 0.784431;    with 0.1 =>0.784731\n",
    "\n",
    "    \n",
    "#linear with 0.2 coef =>   0.701796\n",
    "\n",
    "#   degree 1.2  coef 0.2   =>  0.724251\n",
    "\n",
    "# degree 1.4   coef  0.2  => 0.724251\n",
    "\n",
    "# degree 1.6          0.724251\n",
    "\n",
    "#1.8          0.724251\n",
    "#1.9           0.724251\n",
    "#2.0         0.744910\n",
    "#2.1           0.755240 \n",
    "#2.2          0.761437\n",
    "#2.3          0.765569 \n",
    "#2.4           0.768520\n",
    "#2.5            0.770734\n",
    "#2.6           0.772455\n",
    "#2.7           0.786228\n",
    "#2.8            0.786228\n",
    "#2.9          0.786228\n",
    "#3               0.782859\n",
    "#3.1           0.780838\n",
    "#3.2              0.779491\n",
    "#3.3              0.778529\n",
    "#3.4           0.777807\n",
    "#3.5           0.777246\n",
    "#3.6            0.776796\n",
    "#3.7            0.772754\n",
    "\n",
    "#3.8               0.772754\n",
    "#3.9              0.772754\n",
    "#4.0            0.772231\n",
    "#                      0.771916\n",
    "#                  0.771707 \n",
    "#                 0.771557\n",
    "#                 0.771445 \n",
    "#                 0.771357 \n",
    "#                 0.771287\n",
    "#                 0.771230\n",
    "#                 0.771183\n",
    "#                 0.771142\n",
    "#                 0.770488\n",
    "#      degree 5.1    0.769920\n",
    "#                   0.769424\n",
    "#                    0.768986\n",
    "#                   0.768596\n",
    "#                 0.768248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = np.linspace(3.7, 5.5, 19)\n",
    "# print(degree)\n",
    "\n",
    "coefs = np.linspace(0.5, 5, 46)\n",
    "print(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging of 21 SVM base classifiers with fine-tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_estimators = [9,11,13]\n",
    "n_samples, _,_ = agg_epochs_std.shape\n",
    "labels = agg_labels\n",
    "\n",
    "agg_epochs_std_train_test = agg_epochs_std\n",
    "epochs_data = agg_epochs_std_train_test\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    for train_idx, test_idx in cv.split(epochs_data):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = epochs_data[train_idx]\n",
    "        X_test = epochs_data[test_idx]\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "\n",
    "\n",
    "        cov =  pyriemann.estimation.Covariances('lwf')\n",
    "        cov_X_train = cov.transform(X_train)\n",
    "        cov_X_test = cov.transform(X_test)\n",
    "\n",
    "        n_samples_train, _,_ = cov_X_train.shape\n",
    "        subset_size = n_samples_train//3 * 2\n",
    "\n",
    "        ind_sets = [np.random.randint(0, n_samples_train, size = subset_size) for i in range(n_estimator)]\n",
    "\n",
    "        clfs = [pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', degree=2.7, coef0=0.2,  random_state=0, gamma= 0.03, C=100, probability=True)) for i in range(n_estimator)]\n",
    "\n",
    "\n",
    "        for i in range(n_estimator):\n",
    "            clfs[i].fit(cov_X_train[ind_sets[i],:,:], y_train[ind_sets[i]])\n",
    "\n",
    "        y_predict = [clfs[i].predict(cov_X_test) for i in range(n_estimator)]\n",
    "        y_predict = scipy.stats.mode(y_predict, axis=0).mode[0]\n",
    "\n",
    "\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "        print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for two stage classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_estimator = 9\n",
    "n_samples, _,_ = agg_epochs_std.shape\n",
    "labels = agg_labels\n",
    "\n",
    "agg_epochs_std_train_test = agg_epochs_std\n",
    "epochs_data = agg_epochs_std_train_test\n",
    "\n",
    "wrong = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    val_idx = test_idx[len(test_idx)//2:]\n",
    "    test_idx = test_idx[:len(test_idx)//2]\n",
    "    X_validation = epochs_data[val_idx]\n",
    "    y_validation = np.asarray(labels)[val_idx]\n",
    "    \n",
    "    print(len(val_idx))\n",
    "    \n",
    "    \n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    \n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "#     n_samples_train, _,_ = cov_X_train.shape\n",
    "#     subset_size = n_samples_train//3 * 2\n",
    "    \n",
    "#     ind_sets = [np.random.randint(0, n_samples_train, size = subset_size) for i in range(n_estimator)]\n",
    "    \n",
    "#     clfs = [pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True)) for i in range(n_estimator)]\n",
    "    clf = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "#     clf = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "\n",
    "#     for i in range(n_estimator):\n",
    "#         clfs[i].fit(cov_X_train[ind_sets[i],:,:], y_train[ind_sets[i]])\n",
    "    \n",
    "#     y_predict = [clfs[i].predict(cov_X_test) for i in range(n_estimator)]\n",
    "#     y_predict = scipy.stats.mode(y_predict, axis=0).mode[0]\n",
    "\n",
    "    clf.fit(cov_X_train, y_train)\n",
    "    y_predict = clf.predict(cov_X_test)\n",
    "\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    \n",
    "    \n",
    "#     wrong.append([test_idx[i] for i in range(len(y_test)) if y_predict[i] != y_test[i]])\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick out the channel that were repeatedly classified wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [1128, 1268, 833, 1090, 983, 608, 625, 1201, 602, 901, 1061, 1117, 777, 785, 1295, 680, 903, 724, 826, 296, 1178, 1155, 675, 470, 771, 744, 247, 554, 295, 715, 656, 1051, 1243, 369, 490, 858, 1143, 879, 643, 1272, 1209, 931, 505, 835, 676, 580, 1286, 1324, 1127, 21, 1033, 765, 576, 738, 1180, 760, 670, 81, 1222, 1185, 1182, 839, 743, 723, 254, 429, 936, 1115, 716, 471, 315, 1126, 463, 1186, 184, 244, 1179, 682, 1109, 283, 704, 1247, 1257, 485, 1314, 1057, 1005, 944, 425, 472, 1104, 996, 1161, 1080, 834, 577, 975, 1015, 1008, 1176, 825, 1192, 1083, 1248, 1320, 898, 790, 503, 836, 831, 1184, 1242, 1274, 371, 650, 1141, 655, 1079, 1043, 838, 1211, 755, 436, 874, 1029, 1053, 518, 719, 599, 491, 1147, 997, 1049, 1167, 904, 811, 803, 1157, 1018, 872, 354, 638, 717, 1082, 991, 837, 1025, 37, 1170, 805]\n",
    "result = result + sum(wrong, [])\n",
    "from collections import Counter\n",
    "l = [i[0] for i in list(Counter(result).most_common(150))]\n",
    "print(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training another classifier to classify \"good\" epochs and \"bad\" epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "correct = [i for i in np.random.randint(0,(n_samples//5 * 4),500) if i not in l]\n",
    "bad = l\n",
    "ind = correct + bad\n",
    "X = agg_epochs_std_train_test[ind]\n",
    "y = [0 for i in range(len(correct))] + [1 for i in range(len(bad))] \n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = X\n",
    "labels = y\n",
    "\n",
    "\n",
    "inter_adj = [0, 1.1526818432767743, 0.4054674414465533, -0.057156080501559604, -0.4054627747697755, -0.6931448472215563, -0.9444592755024624, -1.1727179274834425, -1.3862920277815018, -1.591086440427515, -1.791757135889666, -1.9924278313518173]\n",
    "\n",
    "for adj in inter_adj:\n",
    "    print(adj)\n",
    "    for train_idx, test_idx in cv.split(epochs_data):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = epochs_data[train_idx]\n",
    "        X_test = epochs_data[test_idx]\n",
    "        \n",
    "        print(X_test.shape)\n",
    "\n",
    "\n",
    "        cov =  pyriemann.estimation.Covariances('lwf')\n",
    "        cov_X_train = cov.transform(X_train)\n",
    "        cov_X_test = cov.transform(X_test)\n",
    "\n",
    "\n",
    "        ts = TangentSpace()\n",
    "        ts.fit(cov_X_train, y_train)\n",
    "        T = ts.transform(cov_X_train)\n",
    "\n",
    "\n",
    "        bad_catcher =LogisticRegression(penalty='l2', C=1.0, fit_intercept=True)\n",
    "        bad_catcher.fit(T, y_train)\n",
    "\n",
    "\n",
    "        bad_catcher.intercept_[0] = bad_catcher.intercept_[0] - adj\n",
    "        \n",
    "        \n",
    "        T_test = ts.transform(cov_X_test)\n",
    "\n",
    "        y_predict = bad_catcher.predict(T_test)\n",
    "        \n",
    "        \n",
    "#         print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "        true_negative, false_negative, false_positive, true_positive = confusion_matrix(y_test,y_predict)[0][0],confusion_matrix(y_test,y_predict)[1][0],confusion_matrix(y_test,y_predict)[0][1],confusion_matrix(y_test,y_predict)[1][1]   \n",
    "        recall = (true_positive/(true_positive+false_negative))\n",
    "        specificity = (true_negative/(true_negative+false_positive))\n",
    "        print(confusion_matrix(y_test,y_predict))\n",
    "        print(\"recall: {}\".format(recall))\n",
    "        print(\"specificiity: {}\".format(specificity))\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two stag classifer on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_good(epochs, threshold = 0.65):\n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_check_bad= cov.transform(epochs)\n",
    "    T_test = ts.transform(cov_check_bad)\n",
    "    bad = bad_catcher.predict(T_test)\n",
    "    bad_proba = bad_catcher.predict_proba(T_test)\n",
    "    for i in range(len(bad)):\n",
    "        if bad[i] == 1: \n",
    "            if bad_proba[i,1] < threshold:\n",
    "                bad[i] = 0\n",
    "                \n",
    "    print(list(bad).count(1))\n",
    "    res = [i for i in range(epochs.shape[0]) if bad[i] == 0]\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "threshold = [1,0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55]\n",
    "\n",
    "for t in threshold:\n",
    "    print(X_validation.shape)\n",
    "    ind = check_good(X_validation, t)\n",
    "\n",
    "\n",
    "    X_validation_ind = X_validation[ind]\n",
    "    print(\"reminding: {}\".format(X_validation_ind.shape[0] / X_validation.shape[0]))\n",
    "\n",
    "    y_validation_ind = y_validation[ind]\n",
    "\n",
    "\n",
    "\n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_validation= cov.transform(X_validation_ind)\n",
    "\n",
    "\n",
    "\n",
    "    y_predict = clf.predict(cov_X_validation)\n",
    "\n",
    "\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_validation_ind))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix:\n",
    "    \n",
    "\n",
    "\n",
    "[(1128, 34), (1268, 32), (1090, 28), (983, 28), (608, 28), (625, 28), (833, 27), (1201, 26), (602, 25), (901, 25), (1061, 24), (771, 24), (1117, 24), (777, 24), (785, 24), (1295, 24), (680, 24), (903, 24), (724, 24), (826, 24), (296, 23), (744, 23), (247, 23), (295, 23), (1178, 23), (1155, 23), (675, 23), (470, 23), (1243, 22), (369, 22), (81, 21), (858, 21), (1185, 21), (1143, 20), (879, 20), (643, 20), (1272, 20), (1209, 20), (554, 20), (931, 20), (505, 20), (835, 20), (676, 20), (580, 20), (1286, 20), (1324, 20), (715, 20), (656, 20), (1127, 20), (1051, 20), (21, 20), (1033, 20), (723, 19), (765, 19), (254, 19), (576, 19), (738, 19), (1180, 19), (760, 19), (936, 19), (670, 19), (1115, 19), (1222, 18), (716, 18), (1182, 18), (315, 18), (839, 18), (463, 18), (1186, 18), (184, 17), (1179, 17), (836, 17), (1109, 17), (743, 17), (283, 17), (704, 16), (1247, 16), (1257, 16), (485, 16), (1314, 16), (1057, 16), (1005, 16), (944, 16), (425, 16), (472, 16), (1104, 16), (996, 16), (1161, 16), (1080, 16), (834, 16), (429, 16), (577, 16), (975, 16), (490, 16), (1015, 16), (1008, 16), (1176, 15), (838, 15), (471, 15), (825, 15), (1192, 15), (1083, 15), (755, 15), (1126, 15), (1248, 15), (874, 15), (1320, 15), (898, 15), (790, 15), (1029, 14), (244, 14), (1053, 14), (1254, 14), (518, 14), (719, 14), (599, 14), (682, 14), (831, 14), (689, 14), (1184, 14), (1242, 14), (1274, 14), (997, 14), (1049, 14), (1167, 14), (371, 13), (650, 13), (1141, 13), (655, 13), (811, 13), (1079, 13), (1043, 13), (803, 13), (1157, 12), (1018, 12), (872, 12), (354, 12), (638, 12), (717, 12), (1082, 12), (991, 12), (837, 12), (1025, 12), (37, 12), (1170, 12), (805, 12), (1211, 12), (701, 12), (786, 12), (276, 12)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1128, 1268, 1090, 983, 833, 608, 625, 1201, 602, 901, 1061, 1117, 777, 785, 1295, 680, 903, 724, 826, 296, 771, 1178, 1155, 675, 470, 744, 247, 295, 1243, 369, 554, 858, 715, 656, 1051, 1143, 81, 879, 643, 1272, 1209, 931, 505, 835, 676, 580, 1286, 1185, 1324, 1127, 21, 1033, 765, 576, 738, 1180, 760, 670, 1222, 723, 254, 1182, 839, 936, 490, 1115, 716, 315, 743, 463, 1186, 184, 1179, 1109, 429, 283, 704, 1247, 1257, 485, 1314, 1057, 471, 1005, 944, 425, 472, 1104, 996, 836, 1161, 1126, 1080, 834, 577, 975, 1015, 1008, 244, 1176, 825, 1192, 682, 1083, 1248, 1320, 898, 790, 838, 755, 831, 1184, 874, 1242, 1274, 1029, 1053, 371, 650, 518, 719, 1141, 655, 599, 997, 1079, 1043, 1049, 1167, 503, 1254, 1211, 811, 436, 803, 1157, 1018, 872, 354, 638, 717, 1082, 991, 837, 1025, 37, 1170, 805, 701, 786]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_data = agg_epochs_std\n",
    "# labels = agg_labels\n",
    "import sklearn\n",
    "import math\n",
    "\n",
    "\n",
    "entropies = []\n",
    "for e in range(agg_epochs_std.shape[0]):\n",
    "    u, s, vh = np.linalg.svd(agg_epochs_std[e,:,:], full_matrices=True)\n",
    "    s = s/np.linalg.norm(s, ord=1)\n",
    "    svd_entropy = sum([-1*(p * math.log(p)) for p in s])\n",
    "    entropies.append(svd_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = np.asarray(entropies).reshape(-1, 1)\n",
    "\n",
    "\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(entropies):\n",
    "    y_train, y_test = np.asarray(agg_labels)[train_idx], np.asarray(agg_labels)[test_idx]\n",
    "    X_train = entropies[train_idx]\n",
    "    X_test = entropies[test_idx]\n",
    "        \n",
    "\n",
    "    clf = sklearn.linear_model.LogisticRegression(C=1e-5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(agg_labels) == np.asarray(agg_labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                                  class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "ind = np.linspace(0,1032, num = 900)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "epochs_ds = agg_epochs_std[:,:,ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_ds\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    ts = TangentSpace()\n",
    "    T = ts.fit_transform(cov_X_train)\n",
    "    \n",
    "#     clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(200, ), random_state=1)\n",
    "    clf.fit(T, y_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    T_test = ts.transform(cov_X_test)\n",
    "    \n",
    "    y_predict = clf.predict(T_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "ind = np.linspace(0,1032, num = 900)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "epochs_ds = agg_epochs_std[:,:,ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_ds\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    ts = TangentSpace()\n",
    "    T = ts.fit_transform(cov_X_train, y_train)\n",
    "    T = rkhs(T)\n",
    "    \n",
    "    print(T.shape)\n",
    "    \n",
    "    clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf.fit(T, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    T_test = ts.fit_transform(cov_X_test)\n",
    "    T_test = rkhs(T_test)\n",
    "    \n",
    "    y_predict = clf.predict(T_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = [[1,1,2],[2,1,3]]\n",
    "# T = np.asarray(T)\n",
    "\n",
    "\n",
    "import random, math\n",
    "def polynomial(T):\n",
    "    res = []\n",
    "    n_samples, n_features = T.shape\n",
    "    for n in range(n_samples):\n",
    "        ret = []\n",
    "        for k in range(100):\n",
    "            ind = [random.uniform(0, 1) for i in range(n_features)]\n",
    "            ind = [2 if i>0.5 else 1 for i in ind]\n",
    "            s = sum([math.pow(T[n,i], ind[i]) for i in range(n_features)])\n",
    "            ret.append(s)\n",
    "        res.append(ret)\n",
    "    return np.asarray(res)\n",
    "    \n",
    "    \n",
    "    \n",
    "# res = polynomial(T)\n",
    "# res = np.asarray(res)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def rkhs_helper(v):\n",
    "    rkhs = []\n",
    "    for i in range(len(v)-300):\n",
    "        for j in range(len(v)-300):\n",
    "            if i == j:\n",
    "                rkhs.append(v[i]*v[j])\n",
    "            else:\n",
    "                rkhs.append(math.sqrt(2) * v[i]*v[j])\n",
    "    return rkhs\n",
    "\n",
    "\n",
    "def rkhs(T):\n",
    "    ret = []\n",
    "    for i in range(T.shape[0]):\n",
    "        ret.append(rkhs_helper(T[i]))\n",
    "    return np.asarray(ret)\n",
    "\n",
    "\n",
    "\n",
    "# print(rkhs(np.asarray([[1,2,3],[4,5,6],[1,2,3]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from pyriemann.tangentspace import FGDA, TangentSpace\n",
    "\n",
    "ind = np.linspace(0,1032, num = 900)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "epochs_ds = agg_epochs_std[:,:,ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_ds\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    ts = TangentSpace()\n",
    "    T = ts.fit_transform(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "    clf.fit(T, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    T_test = ts.fit_transform(cov_X_test)\n",
    "    \n",
    "    y_predict = clf.predict(T_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45, 0.5, 0.55]\n",
    "inter_adj = [np.log((1-t)*0.142857/(t*0.85714)) for t in tau]\n",
    "\n",
    "print(inter_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,3,2,4,0]\n",
    "n = [11,22,33,44,55]\n",
    "print(np.asarray(n)[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
