{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize and preprocessing data from matlab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_file = 'D:\\data_structure_ANM210862.tar\\data_structure_ANM210862\\data_structure_ANM210862_20130628.mat'\n",
    "label_to_save = 'ANM210862_20130628_labels.npy'\n",
    "trace_dir = 'D:\\\\voltage_traces_ANM210862_20130628\\\\voltage_traces_ANM210862_20130628\\\\raw_trace_958_trial_'\n",
    "sparse_epoch_to_save = 'sparse_ANM210862_20130628.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dict = scipy.io.loadmat(data_structure_file)\n",
    "\n",
    "\n",
    "data = mat_dict['obj']\n",
    "data = data['trialTypeMat'][0,0]\n",
    "\n",
    "valid_trials = []\n",
    "labels = []\n",
    "for i in range(len(data[0,:])):\n",
    "    if data[1,i] == 1:\n",
    "        labels.append(1)\n",
    "        valid_trials.append(i+1)\n",
    "    elif data[0,i] == 1:\n",
    "        labels.append(0)\n",
    "        valid_trials.append(i+1)\n",
    "        \n",
    "print(valid_trials)\n",
    "print(labels)\n",
    "# 1 means right, 0 menas left\n",
    "\n",
    "\n",
    "file = open(label_to_save, 'wb')\n",
    "np.save(file, labels)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_trace_127_trial_5.mat\n",
    "\n",
    "epochs = []\n",
    "epochs = np.asarray(epochs)\n",
    "min_timepoint = 1000000\n",
    "\n",
    "for t in valid_trials:\n",
    "    print(t)\n",
    "    trial = scipy.io.loadmat(trace_dir + str(t) + '.mat')['ch_MUA']\n",
    "    timepoint, chan = trial.shape\n",
    "    if min_timepoint > timepoint:\n",
    "        min_timepoint = timepoint\n",
    "    \n",
    "    trial = trial[:min_timepoint, :]\n",
    "    trial = trial.reshape((1, min_timepoint, chan))\n",
    "    print(trial.shape)\n",
    "    if epochs.size == 0:\n",
    "        epochs = trial\n",
    "    else:\n",
    "        epochs = epochs[:, :min_timepoint, :]\n",
    "        epochs = np.concatenate((epochs, trial), axis = 0)\n",
    "    print(epochs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial, timepoint, channel = epochs.shape\n",
    "sparse_ind = [i*100 for i in range(timepoint//100) ]\n",
    "print(len(sparse_ind))\n",
    "sparse_epochs = epochs[:, sparse_ind, :]\n",
    "sparse_epochs = np.swapaxes(sparse_epochs,1,2)\n",
    "\n",
    "\n",
    "file = open(sparse_epoch_to_save, 'wb')\n",
    "np.save(file, sparse_epochs)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 32, 3443)\n",
      "(328, 32, 1033)\n"
     ]
    }
   ],
   "source": [
    "sparse_epochs = np.load('sparse_ANM210861_20130701.npy')\n",
    "labels = np.load('ANM210861_20130701_labels.npy')\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "ind = np.linspace(0,3443-1, num = 1033)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "sparse_epochs = sparse_epochs[:,:, ind]\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "\n",
    "#get rid of flat channel\n",
    "\n",
    "# trial, channel,timepoint = sparse_epochs.shape\n",
    "# flat_chan = [i for i in range(channel) if sparse_epochs[0,i,0] == 0]\n",
    "# good_chan = [i for i in range(0,32) if i not in flat_chan]\n",
    "# print(flat_chan)\n",
    "# sparse_epochs = sparse_epochs[:,good_chan,:]\n",
    "# print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "#standardize signal based on each channel\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import copy\n",
    "\n",
    "# epochs_std = copy.copy(sparse_epochs)\n",
    "# sample_num, chan_num, timepoint = sparse_epochs.shape\n",
    "# for c in range(chan_num):\n",
    "#     original_timepoints = sparse_epochs[:,c,:]\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(original_timepoints)\n",
    "#     chan_std = scaler.transform(original_timepoints)\n",
    "#     epochs_std[:,c,:] = chan_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on single session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 32, 1033)\n",
      "0.8969465648854962\n",
      "(262, 32, 1033)\n",
      "0.8969465648854962\n",
      "(262, 32, 1033)\n",
      "0.9045801526717557\n",
      "(262, 32, 1033)\n",
      "0.8969465648854962\n",
      "(262, 32, 1033)\n",
      "0.8893129770992366\n",
      "(262, 32, 1033)\n",
      "0.8854961832061069\n",
      "(262, 32, 1033)\n",
      "0.9083969465648855\n",
      "(262, 32, 1033)\n",
      "0.8969465648854962\n",
      "(262, 32, 1033)\n",
      "0.8816793893129771\n",
      "(262, 32, 1033)\n",
      "0.9045801526717557\n",
      "Classification accuracy: 0.860606 / Chance level: 0.509146\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = sparse_epochs\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "\n",
    "for n_component in range(30,31):\n",
    "    sparse_epochs = sparse_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    pca.fit(sparse_epochs)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    new_X = pca.transform(sparse_epochs)\n",
    "\n",
    "    samples = new_X\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 32, 3)\n",
      "(328, 96)\n",
      "Classification accuracy: 0.750000 / Chance level: 0.484907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kun\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 32, 4)\n",
      "(328, 128)\n",
      "Classification accuracy: 0.769697 / Chance level: 0.484907\n",
      "(328, 32, 5)\n",
      "(328, 160)\n",
      "Classification accuracy: 0.780303 / Chance level: 0.484907\n",
      "(328, 32, 6)\n",
      "(328, 192)\n",
      "Classification accuracy: 0.769697 / Chance level: 0.484907\n",
      "(328, 32, 7)\n",
      "(328, 224)\n",
      "Classification accuracy: 0.751515 / Chance level: 0.484907\n",
      "(328, 32, 8)\n",
      "(328, 256)\n",
      "Classification accuracy: 0.792424 / Chance level: 0.484907\n",
      "(328, 32, 9)\n",
      "(328, 288)\n",
      "Classification accuracy: 0.742424 / Chance level: 0.484907\n",
      "(328, 32, 10)\n",
      "(328, 320)\n",
      "Classification accuracy: 0.713636 / Chance level: 0.484907\n",
      "(328, 32, 11)\n",
      "(328, 352)\n",
      "Classification accuracy: 0.681818 / Chance level: 0.484907\n"
     ]
    }
   ],
   "source": [
    "for n_components in range(3,12):\n",
    "    samples = []\n",
    "    for s in range(sparse_epochs.shape[0]):\n",
    "        X = sparse_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((sparse_epochs.shape[0], -1))\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    labels = agg_labels\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from mne import Epochs, pick_types, find_events\n",
    "from mne.channels import read_layout\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "\n",
    "#you need to make sure there is no flat channel! otherwise covariance matrix is not positive definite\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(epochs_std):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_std[train_idx]\n",
    "    X_test = epochs_std[test_idx]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False, cov_est = 'epoch', cov_method_params= 'shrinkage')\n",
    "    new_epochs = csp.fit_transform(X_train,  y_train)\n",
    "    print(new_epochs.shape)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(new_epochs, y_train)\n",
    "    \n",
    "#     logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#     logistic.fit(new_epochs, y_train)\n",
    "    \n",
    "\n",
    "    y_pred_csp = csp.transform(X_test)\n",
    "    y_pred = lda.predict(y_pred_csp)\n",
    "#     y_predict = logistic.predict(y_pred_csp)\n",
    "    \n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aggregate data across session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 32, 1033)\n",
      "(1027,)\n"
     ]
    }
   ],
   "source": [
    "sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "               'sparse_ANM210861_20130703.npy']\n",
    "\n",
    "label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "              'ANM210861_20130703_labels.npy']\n",
    "\n",
    "\n",
    "agg_spares_epochs = np.load(sparse_files[0])\n",
    "agg_labels = np.load(label_files[0])\n",
    "ind = np.linspace(0,3443-1, num = 1033)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "agg_spares_epochs = agg_spares_epochs[:,:, ind]\n",
    "\n",
    "\n",
    "for i in range(1,3):\n",
    "    new_epochs =  np.load(sparse_files[i])\n",
    "    agg_spares_epochs = np.concatenate((agg_spares_epochs, new_epochs), axis = 0)\n",
    "    new_label = np.load(label_files[i])\n",
    "    agg_labels = np.concatenate((agg_labels, new_label), axis = 0)\n",
    "\n",
    "print(agg_spares_epochs.shape)\n",
    "print(agg_labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using Riemannian based classifier on mix dataset across session from same subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821, 32, 1033)\n",
      "0.7990255785627284\n",
      "(821, 32, 1033)\n",
      "0.7917174177831913\n",
      "(821, 32, 1033)\n",
      "0.7892813641900122\n",
      "(821, 32, 1033)\n",
      "0.8087697929354446\n",
      "(821, 32, 1033)\n",
      "0.7929354445797807\n",
      "(821, 32, 1033)\n",
      "0.7941534713763703\n",
      "(821, 32, 1033)\n",
      "0.784409257003654\n",
      "(821, 32, 1033)\n",
      "0.7953714981729598\n",
      "(821, 32, 1033)\n",
      "0.8063337393422655\n",
      "(821, 32, 1033)\n",
      "0.7917174177831913\n",
      "Classification accuracy: 0.736893 / Chance level: 0.484907\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_spares_epochs\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using PCA + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052429577348040116\n",
      "Classification accuracy: 0.515049 / Chance level: 0.484907\n",
      "0.05701270750361295\n",
      "Classification accuracy: 0.514078 / Chance level: 0.484907\n",
      "0.061464911647578814\n",
      "Classification accuracy: 0.513107 / Chance level: 0.484907\n",
      "0.06556584584671517\n",
      "Classification accuracy: 0.514563 / Chance level: 0.484907\n",
      "0.06986368611392207\n",
      "Classification accuracy: 0.515534 / Chance level: 0.484907\n",
      "0.07394506213444246\n",
      "Classification accuracy: 0.514078 / Chance level: 0.484907\n",
      "0.0779305953120939\n",
      "Classification accuracy: 0.505340 / Chance level: 0.484907\n",
      "0.08199828336941267\n",
      "Classification accuracy: 0.506796 / Chance level: 0.484907\n",
      "0.08557103062888351\n",
      "Classification accuracy: 0.512621 / Chance level: 0.484907\n",
      "0.08958147689395413\n",
      "Classification accuracy: 0.512136 / Chance level: 0.484907\n",
      "0.09321724253893537\n",
      "Classification accuracy: 0.504369 / Chance level: 0.484907\n",
      "0.09683000091046615\n",
      "Classification accuracy: 0.523786 / Chance level: 0.484907\n",
      "0.1005017045261865\n",
      "Classification accuracy: 0.504854 / Chance level: 0.484907\n",
      "0.10428354718590707\n",
      "Classification accuracy: 0.503398 / Chance level: 0.484907\n",
      "0.1076381602645758\n",
      "Classification accuracy: 0.504369 / Chance level: 0.484907\n",
      "0.11105888358889544\n",
      "Classification accuracy: 0.514078 / Chance level: 0.484907\n",
      "0.11472498659440826\n",
      "Classification accuracy: 0.510194 / Chance level: 0.484907\n",
      "0.11832238409117003\n",
      "Classification accuracy: 0.500485 / Chance level: 0.484907\n",
      "0.12160913312446703\n",
      "Classification accuracy: 0.506311 / Chance level: 0.484907\n",
      "0.12486148699500539\n",
      "Classification accuracy: 0.496117 / Chance level: 0.484907\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "trial,_,_ = agg_spares_epochs.shape\n",
    "for n_component in range(10,30):\n",
    "    epochs_reshaped = agg_spares_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    samples = pca.fit_transform(epochs_reshaped)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    \n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction Using PCA + logistic across channels on aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 32, 3)\n",
      "(1027, 96)\n",
      "Classification accuracy: 0.573301 / Chance level: 0.484907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kun\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 32, 4)\n",
      "(1027, 128)\n",
      "Classification accuracy: 0.604854 / Chance level: 0.484907\n",
      "(1027, 32, 5)\n",
      "(1027, 160)\n",
      "Classification accuracy: 0.634951 / Chance level: 0.484907\n",
      "(1027, 32, 6)\n",
      "(1027, 192)\n",
      "Classification accuracy: 0.654369 / Chance level: 0.484907\n",
      "(1027, 32, 7)\n",
      "(1027, 224)\n",
      "Classification accuracy: 0.650485 / Chance level: 0.484907\n",
      "(1027, 32, 8)\n",
      "(1027, 256)\n",
      "Classification accuracy: 0.662136 / Chance level: 0.484907\n",
      "(1027, 32, 9)\n",
      "(1027, 288)\n",
      "Classification accuracy: 0.658252 / Chance level: 0.484907\n",
      "(1027, 32, 10)\n",
      "(1027, 320)\n",
      "Classification accuracy: 0.655825 / Chance level: 0.484907\n",
      "(1027, 32, 11)\n",
      "(1027, 352)\n",
      "Classification accuracy: 0.655340 / Chance level: 0.484907\n",
      "(1027, 32, 12)\n",
      "(1027, 384)\n",
      "Classification accuracy: 0.643689 / Chance level: 0.484907\n",
      "(1027, 32, 13)\n",
      "(1027, 416)\n",
      "Classification accuracy: 0.650000 / Chance level: 0.484907\n",
      "(1027, 32, 14)\n",
      "(1027, 448)\n",
      "Classification accuracy: 0.652913 / Chance level: 0.484907\n",
      "(1027, 32, 15)\n",
      "(1027, 480)\n",
      "Classification accuracy: 0.647573 / Chance level: 0.484907\n",
      "(1027, 32, 16)\n",
      "(1027, 512)\n",
      "Classification accuracy: 0.629612 / Chance level: 0.484907\n",
      "(1027, 32, 17)\n",
      "(1027, 544)\n",
      "Classification accuracy: 0.639320 / Chance level: 0.484907\n",
      "(1027, 32, 18)\n",
      "(1027, 576)\n",
      "Classification accuracy: 0.632524 / Chance level: 0.484907\n",
      "(1027, 32, 19)\n",
      "(1027, 608)\n",
      "Classification accuracy: 0.624757 / Chance level: 0.484907\n",
      "(1027, 32, 20)\n",
      "(1027, 640)\n",
      "Classification accuracy: 0.612136 / Chance level: 0.484907\n",
      "(1027, 32, 21)\n",
      "(1027, 672)\n",
      "Classification accuracy: 0.599515 / Chance level: 0.484907\n",
      "(1027, 32, 22)\n",
      "(1027, 704)\n",
      "Classification accuracy: 0.595146 / Chance level: 0.484907\n",
      "(1027, 32, 23)\n",
      "(1027, 736)\n",
      "Classification accuracy: 0.601456 / Chance level: 0.484907\n",
      "(1027, 32, 24)\n",
      "(1027, 768)\n",
      "Classification accuracy: 0.577670 / Chance level: 0.484907\n",
      "(1027, 32, 25)\n",
      "(1027, 800)\n",
      "Classification accuracy: 0.570388 / Chance level: 0.484907\n",
      "(1027, 32, 26)\n",
      "(1027, 832)\n",
      "Classification accuracy: 0.535437 / Chance level: 0.484907\n",
      "(1027, 32, 27)\n",
      "(1027, 864)\n",
      "Classification accuracy: 0.538350 / Chance level: 0.484907\n",
      "(1027, 32, 28)\n",
      "(1027, 896)\n",
      "Classification accuracy: 0.520388 / Chance level: 0.484907\n",
      "(1027, 32, 29)\n",
      "(1027, 928)\n",
      "Classification accuracy: 0.522816 / Chance level: 0.484907\n"
     ]
    }
   ],
   "source": [
    "for n_components in range(3,30):\n",
    "    samples = []\n",
    "    for s in range(agg_spares_epochs.shape[0]):\n",
    "        X = agg_spares_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((agg_spares_epochs.shape[0], -1))\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    labels = agg_labels\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
