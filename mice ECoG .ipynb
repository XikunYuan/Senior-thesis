{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize and preprocessing data from matlab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_file = 'D:\\data_structure_ANM210862.tar\\data_structure_ANM210862\\data_structure_ANM210862_20130628.mat'\n",
    "label_to_save = 'ANM210862_20130628_labels.npy'\n",
    "trace_dir = 'D:\\\\voltage_traces_ANM210862_20130628\\\\voltage_traces_ANM210862_20130628\\\\raw_trace_958_trial_'\n",
    "sparse_epoch_to_save = 'sparse_ANM210862_20130628.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dict = scipy.io.loadmat(data_structure_file)\n",
    "\n",
    "\n",
    "data = mat_dict['obj']\n",
    "data = data['trialTypeMat'][0,0]\n",
    "\n",
    "valid_trials = []\n",
    "labels = []\n",
    "for i in range(len(data[0,:])):\n",
    "    if data[1,i] == 1:\n",
    "        labels.append(1)\n",
    "        valid_trials.append(i+1)\n",
    "    elif data[0,i] == 1:\n",
    "        labels.append(0)\n",
    "        valid_trials.append(i+1)\n",
    "        \n",
    "print(valid_trials)\n",
    "print(labels)\n",
    "# 1 means right, 0 menas left\n",
    "\n",
    "\n",
    "file = open(label_to_save, 'wb')\n",
    "np.save(file, labels)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_trace_127_trial_5.mat\n",
    "\n",
    "epochs = []\n",
    "epochs = np.asarray(epochs)\n",
    "min_timepoint = 1000000\n",
    "\n",
    "for t in valid_trials:\n",
    "    print(t)\n",
    "    trial = scipy.io.loadmat(trace_dir + str(t) + '.mat')['ch_MUA']\n",
    "    timepoint, chan = trial.shape\n",
    "    if min_timepoint > timepoint:\n",
    "        min_timepoint = timepoint\n",
    "    \n",
    "    trial = trial[:min_timepoint, :]\n",
    "    trial = trial.reshape((1, min_timepoint, chan))\n",
    "    print(trial.shape)\n",
    "    if epochs.size == 0:\n",
    "        epochs = trial\n",
    "    else:\n",
    "        epochs = epochs[:, :min_timepoint, :]\n",
    "        epochs = np.concatenate((epochs, trial), axis = 0)\n",
    "    print(epochs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial, timepoint, channel = epochs.shape\n",
    "sparse_ind = [i*100 for i in range(timepoint//100) ]\n",
    "print(len(sparse_ind))\n",
    "sparse_epochs = epochs[:, sparse_ind, :]\n",
    "sparse_epochs = np.swapaxes(sparse_epochs,1,2)\n",
    "\n",
    "\n",
    "file = open(sparse_epoch_to_save, 'wb')\n",
    "np.save(file, sparse_epochs)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 32, 1033)\n",
      "[0, 2, 3, 7, 10, 14, 15, 20, 22, 23, 25, 27, 30, 31]\n",
      "(317, 18, 1033)\n"
     ]
    }
   ],
   "source": [
    "sparse_epochs = np.load('sparse_ANM210861_20130702.npy')\n",
    "labels = np.load('ANM210861_20130702_labels.npy')\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "# ind = np.linspace(0,3443-1, num = 300)\n",
    "# ind = [int(np.floor(i)) for i in ind]\n",
    "# sparse_epochs = sparse_epochs[:,:, ind]\n",
    "# print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "\n",
    "#get rid of flat channel\n",
    "\n",
    "trial, channel,timepoint = sparse_epochs.shape\n",
    "flat_chan = [i for i in range(channel) if sparse_epochs[0,i,0] == 0]\n",
    "good_chan = [i for i in range(0,32) if i not in flat_chan]\n",
    "print(flat_chan)\n",
    "sparse_epochs = sparse_epochs[:,good_chan,:]\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "#standardize signal based on each channel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "epochs_std = copy.copy(sparse_epochs)\n",
    "sample_num, chan_num, timepoint = sparse_epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = sparse_epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    epochs_std[:,c,:] = chan_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on single session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 18, 18)\n",
      "0.924901185770751\n",
      "(253, 18, 18)\n",
      "0.9209486166007905\n",
      "(253, 18, 18)\n",
      "0.9288537549407114\n",
      "(253, 18, 18)\n",
      "0.9288537549407114\n",
      "(253, 18, 18)\n",
      "0.9169960474308301\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.932806324110672\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.924901185770751\n",
      "Classification accuracy: 0.901563 / Chance level: 0.457413\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_std\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    print(cov_X_train.shape)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=sklearn.discriminant_analysis.LinearDiscriminantAnalysis())\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PCA across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "for n_component in range(30,31):\n",
    "    sparse_epochs_rs = sparse_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    pca.fit(sparse_epochs_rs)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    new_X = pca.transform(sparse_epochs_rs)\n",
    "\n",
    "    samples = new_X\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PCA across channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "for n_components in range(3,12):\n",
    "    samples = []\n",
    "    for s in range(sparse_epochs.shape[0]):\n",
    "        X = sparse_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((sparse_epochs.shape[0], -1))\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1]\n",
      "[1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1\n",
      " 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1]\n",
      "0.803030303030303\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
      " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0]\n",
      "[1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
      " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0]\n",
      "0.8787878787878788\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0]\n",
      "[1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0]\n",
      "0.8636363636363636\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
      " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1]\n",
      "[0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
      " 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1]\n",
      "0.7878787878787878\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0]\n",
      "[0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0]\n",
      "0.8484848484848485\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1]\n",
      "[1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1]\n",
      "0.8939393939393939\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1]\n",
      "0.803030303030303\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0]\n",
      "0.8181818181818182\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      "[0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      "0.8636363636363636\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
      " 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1]\n",
      "0.7878787878787878\n",
      "Classification accuracy: 0.834848 / Chance level: 0.509146\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from mne import Epochs, pick_types, find_events\n",
    "from mne.channels import read_layout\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "\n",
    "#you need to make sure there is no flat channel! otherwise covariance matrix is not positive definite\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(epochs_std):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_std[train_idx]\n",
    "    X_test = epochs_std[test_idx]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False, cov_est = 'epoch', cov_method_params= 'shrinkage')\n",
    "    new_epochs = csp.fit_transform(X_train,  y_train)\n",
    "    print(new_epochs.shape)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(new_epochs, y_train)\n",
    "    \n",
    "#     logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#     logistic.fit(new_epochs, y_train)\n",
    "    \n",
    "\n",
    "    y_pred_csp = csp.transform(X_test)\n",
    "    y_pred = lda.predict(y_pred_csp)\n",
    "#     y_predict = logistic.predict(y_pred_csp)\n",
    "    \n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aggregate data across session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1669, 32, 1033)\n",
      "(1669,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "# sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "#                'sparse_ANM210861_20130703.npy']\n",
    "\n",
    "# label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "#               'ANM210861_20130703_labels.npy']\n",
    "\n",
    "sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "               'sparse_ANM210861_20130703.npy', 'sparse_ANM210862_20130626.npy', 'sparse_ANM210862_20130627.npy', \n",
    "               'sparse_ANM210862_20130628.npy']\n",
    "\n",
    "label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "              'ANM210861_20130703_labels.npy', 'ANM210862_20130626_labels.npy', 'ANM210862_20130627_labels.npy',\n",
    "              'ANM210862_20130628_labels.npy']\n",
    "\n",
    "\n",
    "agg_spares_epochs = np.load(sparse_files[0])\n",
    "agg_labels = np.load(label_files[0])\n",
    "ind = np.linspace(0,3443-1, num = 1033)\n",
    "ind = [int(np.floor(i)) for i in ind]\n",
    "agg_spares_epochs = agg_spares_epochs[:,:, ind]\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    new_epochs =  np.load(sparse_files[i])\n",
    "    agg_spares_epochs = np.concatenate((agg_spares_epochs, new_epochs), axis = 0)\n",
    "    new_label = np.load(label_files[i])\n",
    "    agg_labels = np.concatenate((agg_labels, new_label), axis = 0)\n",
    "\n",
    "print(agg_spares_epochs.shape)\n",
    "print(agg_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "agg_epochs_std = copy.copy(agg_spares_epochs)\n",
    "sample_num, chan_num, timepoint = agg_spares_epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = agg_spares_epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    agg_epochs_std[:,c,:] = chan_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using Riemannian based classifier on mix dataset across session from same subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "160\n",
      "(1335, 32, 1033)\n",
      "0.9820224719101124\n",
      "334\n",
      "160\n",
      "(1335, 32, 1033)\n",
      "0.9842696629213483\n",
      "334\n",
      "150\n",
      "(1335, 32, 1033)\n",
      "0.9827715355805243\n",
      "334\n",
      "146\n",
      "(1335, 32, 1033)\n",
      "0.9820224719101124\n",
      "334\n",
      "145\n",
      "(1335, 32, 1033)\n",
      "0.9805243445692884\n",
      "334\n",
      "154\n",
      "(1335, 32, 1033)\n",
      "0.9865168539325843\n",
      "334\n",
      "150\n",
      "(1335, 32, 1033)\n",
      "0.9805243445692884\n",
      "334\n",
      "139\n",
      "(1335, 32, 1033)\n",
      "0.9865168539325843\n",
      "334\n",
      "145\n",
      "(1335, 32, 1033)\n",
      "0.9820224719101124\n",
      "334\n",
      "151\n",
      "(1335, 32, 1033)\n",
      "0.9835205992509364\n",
      "Classification accuracy: 0.857189 / Chance level: 0.500899\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_good(agg_epochs_std, test_idx):\n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    res = [i for i in test_idx if (bad_catcher.predict(cov.transform(agg_epochs_std[[i]]))[0] == 0)]\n",
    "    return res\n",
    "\n",
    "\n",
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    print(len(test_idx))\n",
    "    test_idx = check_good(epochs_data, test_idx)\n",
    "    print(len(test_idx))\n",
    "    \n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=sklearn.discriminant_analysis.LinearDiscriminantAnalysis())\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100))\n",
    "     \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=LogisticRegression())\n",
    "\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using PCA + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "trial,_,_ = agg_spares_epochs.shape\n",
    "for n_component in range(10,30):\n",
    "    epochs_reshaped = agg_spares_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    samples = pca.fit_transform(epochs_reshaped)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    \n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction Using PCA + logistic across channels on aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_components in range(3,30):\n",
    "    samples = []\n",
    "    for s in range(agg_spares_epochs.shape[0]):\n",
    "        X = agg_spares_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((agg_spares_epochs.shape[0], -1))\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    labels = agg_labels\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting With MDM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 19, 300)\n",
      "0.7824427480916031\n",
      "(262, 19, 300)\n",
      "0.7977099236641222\n",
      "(262, 19, 300)\n",
      "0.7862595419847328\n",
      "(262, 19, 300)\n",
      "0.7938931297709924\n",
      "(262, 19, 300)\n",
      "0.8053435114503816\n",
      "(262, 19, 300)\n",
      "0.7786259541984732\n",
      "(262, 19, 300)\n",
      "0.7900763358778626\n",
      "(262, 19, 300)\n",
      "0.7633587786259542\n",
      "(262, 19, 300)\n",
      "0.7938931297709924\n",
      "(262, 19, 300)\n",
      "0.8091603053435115\n",
      "Classification accuracy: 0.751515 / Chance level: 0.509146\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = sparse_epochs\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    \n",
    "    MDMclassifier = pyriemann.classification.MDM(metric='riemann')\n",
    "    clf = AdaBoostClassifier(n_estimators=4, base_estimator=MDMclassifier,learning_rate=0.7)\n",
    "    \n",
    "    \n",
    "    clf.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = clf.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(clf.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "#MDM with no boosting: 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9941520467836257\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "1.0\n",
      "(513, 32, 1033)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f78102d6908b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTSclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTangentSpace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\tangentspace.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_reference_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         self.reference_ = mean_covariance(X, metric=self.metric,\n\u001b[1;32m--> 169\u001b[1;33m                                           sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtangent_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreference_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\utils\\mean.py\u001b[0m in \u001b[0;36mmean_covariance\u001b[1;34m(covmats, metric, sample_weight, *args)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_methods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\utils\\mean.py\u001b[0m in \u001b[0;36mmean_riemann\u001b[1;34m(covmats, tol, maxiter, init, sample_weight)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCm12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovmats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCm12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=5, base_estimator=TSclassifier,learning_rate=1)\n",
    "#     \n",
    "    clf.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = clf.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(clf.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "#MDM with no boosting: 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "0.8966861598440545\n",
      "(513, 32, 1033)\n",
      "0.9161793372319688\n",
      "(513, 32, 1033)\n",
      "0.8927875243664717\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8732943469785575\n",
      "(513, 32, 1033)\n",
      "0.9181286549707602\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.9122807017543859\n",
      "Classification accuracy: 0.730233 / Chance level: 0.526480\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    n_samples, _,_ = cov_X_train.shape\n",
    "    ind_set_1 = np.random.randint(0, n_samples, size = n_samples//3 * 2)\n",
    "    ind_set_2 = np.random.randint(0, n_samples, size = n_samples//3 * 2)\n",
    "    ind_set_3 = np.random.randint(0, n_samples, size = n_samples//3*2)\n",
    "\n",
    "    clf_1 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "    clf_2 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "    clf_3 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "\n",
    "    clf_1.fit(cov_X_train[ind_set_1,:,:], y_train[ind_set_1])\n",
    "    clf_2.fit(cov_X_train[ind_set_2,:,:], y_train[ind_set_2])\n",
    "    clf_3.fit(cov_X_train[ind_set_3,:,:], y_train[ind_set_3])\n",
    "\n",
    "    y_predict = scipy.stats.mode([clf_1.predict(cov_X_test), clf_2.predict(cov_X_test), clf_3.predict(cov_X_test)], axis=0).mode[0]\n",
    "    print(sklearn.metrics.accuracy_score(scipy.stats.mode([clf_1.predict(cov_X_train), clf_2.predict(cov_X_train), clf_3.predict(cov_X_train)], axis=0).mode[0], y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694610778443114\n",
      "0.7844311377245509\n",
      "0.7844311377245509\n",
      "0.7934131736526946\n",
      "0.8173652694610778\n",
      "0.7694610778443114\n",
      "0.8053892215568862\n",
      "0.7904191616766467\n",
      "0.8053892215568862\n",
      "0.7934131736526946\n",
      "Classification accuracy: 0.791317 / Chance level: 0.500899\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "n_estimator = 21\n",
    "subset_size = n_samples//3 * 2\n",
    "\n",
    "wrong = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    n_samples, _,_ = cov_X_train.shape\n",
    "    \n",
    "    ind_sets = [np.random.randint(0, n_samples, size = subset_size) for i in range(n_estimator)]\n",
    "    \n",
    "    clfs = [pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True)) for i in range(n_estimator)]\n",
    "\n",
    "    for i in range(n_estimator):\n",
    "        clfs[i].fit(cov_X_train[ind_sets[i],:,:], y_train[ind_sets[i]])\n",
    "    \n",
    "    y_predict = [clfs[i].predict(cov_X_test) for i in range(n_estimator)]\n",
    "    y_predict = scipy.stats.mode(y_predict, axis=0).mode[0]\n",
    "    \n",
    "\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    \n",
    "    print(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    \n",
    "    wrong.append([test_idx[i] for i in range(len(y_test)) if y_predict[i] != y_test[i]])\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1344, 5), (599, 5), (989, 5), (1057, 5), (457, 5), (1115, 5), (1485, 5), (785, 4), (1161, 4), (1468, 4), (432, 4), (834, 4), (485, 4), (1648, 4), (806, 4), (822, 4), (810, 4), (1381, 4), (1503, 4), (675, 4), (811, 4), (1131, 4), (1137, 4), (1645, 4), (782, 3), (1535, 3), (650, 3), (744, 3), (1370, 3), (1179, 3), (879, 3), (836, 3), (1260, 3), (724, 3), (1324, 3), (946, 3), (1353, 3), (247, 3), (670, 3), (184, 3)]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "result = result + sum(wrong, [])\n",
    "\n",
    "# temp = wrong\n",
    "# result = sum(temp, [])\n",
    "from collections import Counter\n",
    "print(Counter(result).most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1344, 5), (599, 5), (989, 5), (1057, 5), (457, 5), (1115, 5), (1485, 5), (785, 4), (1161, 4), (1468, 4), (432, 4), (834, 4), (485, 4), (1648, 4), (806, 4), (822, 4), (810, 4), (1381, 4), (1503, 4), (675, 4), (811, 4), (1131, 4), (1137, 4), (1645, 4), (782, 3), (1535, 3), (650, 3), (744, 3), (1370, 3), (1179, 3), (879, 3), (836, 3), (1260, 3), (724, 3), (1324, 3), (946, 3), (1353, 3), (247, 3), (670, 3), (184, 3), (839, 3), (997, 3), (1567, 3), (1481, 3), (429, 3), (711, 3), (898, 3), (743, 3), (591, 3), (1376, 3), (795, 3), (996, 3), (1025, 3), (874, 3), (835, 3), (295, 3), (1010, 3), (354, 3), (3, 3), (538, 3), (1422, 3), (900, 3), (1362, 3), (826, 3), (1242, 3), (1104, 3), (1268, 3), (423, 3), (1337, 3), (877, 3), (1049, 3), (746, 3), (691, 3), (851, 3), (1407, 3), (1271, 3), (1408, 3), (936, 3), (831, 3), (1366, 3), (1186, 3), (1350, 3), (1522, 3), (282, 3), (801, 3), (1499, 3), (931, 3), (1157, 2), (1080, 2), (576, 2), (1176, 2), (1375, 2), (425, 2), (1295, 2), (115, 2), (1573, 2), (1403, 2), (1054, 2), (567, 2), (700, 2), (439, 2), (1174, 2), (1168, 2), (1113, 2), (297, 2), (903, 2), (1083, 2), (993, 2), (643, 2), (1493, 2), (602, 2), (1626, 2), (1065, 2), (1248, 2), (1510, 2), (1192, 2), (505, 2), (5, 2), (393, 2), (738, 2), (395, 2), (1620, 2), (1433, 2), (1462, 2), (1536, 2), (1429, 2), (296, 2), (828, 2), (1247, 2), (1571, 2), (1008, 2), (992, 2), (577, 2), (1034, 2), (1187, 2), (716, 2), (471, 2), (472, 2), (1155, 2), (1653, 2), (975, 2), (911, 2), (1125, 2), (276, 2), (679, 2), (1189, 2), (1621, 2), (1467, 2), (1009, 2), (1171, 2)]\n",
      "[1344, 599, 989, 1057, 457, 1115, 1485, 785, 1161, 1468, 432, 834, 485, 1648, 806, 822, 810, 1381, 1503, 675, 811, 1131, 1137, 1645, 782, 1535, 650, 744, 1370, 1179, 879, 836, 1260, 724, 1324, 946, 1353, 247, 670, 184, 839, 997, 1567, 1481, 429, 711, 898, 743, 591, 1376, 795, 996, 1025, 874, 835, 295, 1010, 354, 3, 538, 1422, 900, 1362, 826, 1242, 1104, 1268, 423, 1337, 877, 1049, 746, 691, 851, 1407, 1271, 1408, 936, 831, 1366, 1186, 1350, 1522, 282, 801, 1499, 931, 1157, 1080, 576, 1176, 1375, 425, 1295, 115, 1573, 1403, 1054, 567, 700, 439, 1174, 1168, 1113, 297, 903, 1083, 993, 643, 1493, 602, 1626, 1065, 1248, 1510, 1192, 505, 5, 393, 738, 395, 1620, 1433, 1462, 1536, 1429, 296, 828, 1247, 1571, 1008, 992, 577, 1034, 1187, 716, 471, 472, 1155, 1653, 975, 911, 1125, 276, 679, 1189, 1621, 1467, 1009, 1171]\n"
     ]
    }
   ],
   "source": [
    "print(list(Counter(result).most_common(150)))\n",
    "l = [i[0] for i in list(Counter(result).most_common(150))]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03839222 0.96160778]]\n",
      "[[0.31764853 0.68235147]]\n",
      "[[0.82268444 0.17731556]]\n",
      "[[0.22734746 0.77265254]]\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(clfs[0].predict_proba(cov.transform(agg_epochs_std[[1344]])))\n",
    "print(clfs[0].predict_proba(cov.transform(agg_epochs_std[[599]])))\n",
    "print(clfs[0].predict_proba(cov.transform(agg_epochs_std[[989]])))\n",
    "print(clfs[0].predict_proba(cov.transform(agg_epochs_std[[1057]])))\n",
    "\n",
    "# print(clfs[1].predict_proba(cov.transform(agg_epochs_std[[1]])))\n",
    "# print(clfs[1].predict_proba(cov.transform(agg_epochs_std[[2]])))\n",
    "# print(clfs[1].predict_proba(cov.transform(agg_epochs_std[[3]])))\n",
    "# print(clfs[1].predict_proba(cov.transform(agg_epochs_std[[4]])))\n",
    "\n",
    "\n",
    "print(agg_labels[1344])\n",
    "print(agg_labels[599])\n",
    "print(agg_labels[989])\n",
    "print(agg_labels[1057])\n",
    "\n",
    "\n",
    "# print(agg_labels[1])\n",
    "# print(agg_labels[2])\n",
    "# print(agg_labels[3])\n",
    "# print(agg_labels[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# got rid of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n",
      "399\n",
      "[0.689922480620155]\n",
      "513\n",
      "397\n",
      "[0.689922480620155, 0.7751937984496124]\n",
      "513\n",
      "400\n",
      "[0.689922480620155, 0.7751937984496124, 0.6666666666666666]\n",
      "513\n",
      "404\n",
      "[0.689922480620155, 0.7751937984496124, 0.6666666666666666, 0.6976744186046512]\n",
      "513\n",
      "401\n",
      "[0.689922480620155, 0.7751937984496124, 0.6666666666666666, 0.6976744186046512, 0.6976744186046512]\n",
      "Classification accuracy: 0.705426 / Chance level: 0.526480\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "n_estimator = 21\n",
    "subset_size = n_samples//2\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    \n",
    "    print(len(train_idx))\n",
    "    train_idx = [i for i in train_idx if i not in l]\n",
    "    print(len(train_idx))\n",
    "    \n",
    "    \n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    n_samples, _,_ = cov_X_train.shape\n",
    "    \n",
    "    ind_sets = [np.random.randint(0, n_samples, size = subset_size) for i in range(n_estimator)]\n",
    "    \n",
    "    clfs = [pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True)) for i in range(n_estimator)]\n",
    "\n",
    "    for i in range(n_estimator):\n",
    "        clfs[i].fit(cov_X_train[ind_sets[i],:,:], y_train[ind_sets[i]])\n",
    "    \n",
    "    y_predict = [clfs[i].predict(cov_X_test) for i in range(n_estimator)]\n",
    "    y_predict = scipy.stats.mode(y_predict, axis=0).mode[0]\n",
    "    \n",
    "\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    print(scores)\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(546, 26), (134, 24), (22, 22), (63, 21), (298, 21), (90, 20), (386, 20), (6, 19), (56, 19), (233, 19), (498, 19), (152, 19), (376, 19), (10, 18), (424, 18), (155, 18), (77, 18), (621, 18), (544, 18), (245, 18), (495, 18), (241, 18), (116, 18), (2, 17), (572, 17), (5, 17), (603, 17), (53, 17), (564, 17), (508, 17), (264, 16), (284, 16), (220, 16), (244, 16), (100, 16), (401, 16), (96, 16), (192, 15), (496, 15), (101, 15), (440, 15), (483, 15), (162, 15), (309, 15), (143, 15), (536, 15), (49, 14), (567, 14), (381, 14), (201, 14), (128, 14), (464, 14), (34, 14), (594, 14), (120, 14), (559, 14), (268, 13), (174, 13), (30, 13), (551, 13), (504, 13), (285, 13), (293, 13), (27, 13), (297, 13), (168, 13), (175, 12), (184, 12), (529, 12), (454, 12), (354, 12), (452, 12), (458, 12), (349, 12), (472, 12), (130, 12), (420, 12), (232, 12), (474, 12), (88, 12), (282, 12), (221, 12), (392, 12), (476, 12), (310, 12), (609, 11), (618, 11), (410, 11), (337, 11), (348, 11), (326, 11), (519, 11), (331, 11), (435, 11), (157, 11), (173, 11), (39, 10), (494, 10), (364, 10), (159, 10), (51, 10), (521, 10), (402, 10), (273, 10), (79, 10), (35, 10), (339, 10), (531, 10), (160, 10), (38, 10), (416, 10), (343, 10), (323, 10), (607, 10), (182, 10), (133, 9), (560, 9), (254, 9), (406, 9), (290, 9)]\n",
    "\n",
    "\n",
    "[(546, 18), (63, 15), (134, 15), (5, 15), (498, 15), (376, 15), (10, 14), (386, 14), (22, 14), (152, 14), (241, 14), (96, 14), (264, 13), (6, 13), (53, 13), (495, 13), (401, 13), (564, 13), (508, 13), (559, 12), (101, 12), (220, 12), (77, 12), (309, 12), (544, 12), (100, 12), (245, 12), (536, 12), (192, 11), (298, 11), (56, 11), (483, 11), (233, 11), (572, 11), (128, 11), (285, 11), (27, 11), (143, 11), (116, 11), (496, 10), (424, 10), (284, 10), (268, 10), (49, 10), (567, 10), (621, 10), (184, 10), (244, 10), (201, 10), (529, 10), (354, 10), (34, 10), (603, 10), (173, 10), (594, 10), (221, 10), (120, 10), (2, 9), (174, 9), (551, 9), (440, 9), (504, 9), (452, 9), (293, 9), (349, 9), (92, 9), (297, 9), (474, 9), (481, 9), (88, 9), (519, 9), (343, 9), (157, 9), (182, 9), (358, 9), (208, 8), (494, 8), (162, 8), (381, 8), (283, 8), (454, 8), (464, 8), (458, 8), (472, 8), (420, 8), (326, 8), (121, 8), (28, 8), (339, 8), (323, 8), (38, 8), (155, 8), (476, 8), (99, 8), (435, 8), (13, 8), (112, 8), (71, 8), (90, 7), (473, 7), (364, 7), (618, 7), (406, 7), (402, 7), (368, 7), (130, 7), (147, 7), (337, 7), (348, 7), (232, 7), (513, 7), (411, 7), (409, 7), (331, 7), (149, 7), (282, 7), (168, 7), (113, 7), (144, 7), (607, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(651, 32, 1033)\n",
      "0.9354838709677419\n",
      "(651, 32, 1033)\n",
      "0.9339477726574501\n",
      "(651, 32, 1033)\n",
      "0.9493087557603687\n",
      "(651, 32, 1033)\n",
      "0.9400921658986175\n",
      "(651, 32, 1033)\n",
      "0.9370199692780338\n",
      "(651, 32, 1033)\n",
      "0.9385560675883257\n",
      "(651, 32, 1033)\n",
      "0.9324116743471582\n",
      "(651, 32, 1033)\n",
      "0.9385560675883257\n",
      "(651, 32, 1033)\n",
      "0.9416282642089093\n",
      "(651, 32, 1033)\n",
      "0.9293394777265745\n",
      "Classification accuracy: 0.922086 / Chance level: 0.815725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "correct = [i for i in range(700) if i not in l]\n",
    "bad = l\n",
    "ind = correct + bad\n",
    "X = agg_epochs_std[ind]\n",
    "y = [0 for i in range(len(correct))] + [1 for i in range(len(bad))] \n",
    "\n",
    "\n",
    "# cov =  pyriemann.estimation.Covariances('lwf')\n",
    "# cov_X = cov.transform(X)\n",
    "\n",
    "    \n",
    "    \n",
    "# TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "\n",
    "\n",
    "# TSclassifier.fit(cov_X, y)\n",
    "\n",
    "\n",
    "# y_predict = TSclassifier.predict(cov_X)\n",
    "# print((sklearn.metrics.accuracy_score(y_predict, y)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "epochs_data = X\n",
    "labels = y\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    \n",
    "    bad_catcher = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "\n",
    "    bad_catcher.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = bad_catcher.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(bad_catcher.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "y_predict = bad_catcher.predict(cov.transform(agg_epochs_std[[1344]]))\n",
    "print(y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
