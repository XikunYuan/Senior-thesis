{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize and preprocessing data from matlab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_file = 'D:\\data_structure_ANM210862.tar\\data_structure_ANM210862\\data_structure_ANM210862_20130628.mat'\n",
    "label_to_save = 'ANM210862_20130628_labels.npy'\n",
    "trace_dir = 'D:\\\\voltage_traces_ANM210862_20130628\\\\voltage_traces_ANM210862_20130628\\\\raw_trace_958_trial_'\n",
    "sparse_epoch_to_save = 'sparse_ANM210862_20130628.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dict = scipy.io.loadmat(data_structure_file)\n",
    "\n",
    "\n",
    "data = mat_dict['obj']\n",
    "data = data['trialTypeMat'][0,0]\n",
    "\n",
    "valid_trials = []\n",
    "labels = []\n",
    "for i in range(len(data[0,:])):\n",
    "    if data[1,i] == 1:\n",
    "        labels.append(1)\n",
    "        valid_trials.append(i+1)\n",
    "    elif data[0,i] == 1:\n",
    "        labels.append(0)\n",
    "        valid_trials.append(i+1)\n",
    "        \n",
    "print(valid_trials)\n",
    "print(labels)\n",
    "# 1 means right, 0 menas left\n",
    "\n",
    "\n",
    "file = open(label_to_save, 'wb')\n",
    "np.save(file, labels)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_trace_127_trial_5.mat\n",
    "\n",
    "epochs = []\n",
    "epochs = np.asarray(epochs)\n",
    "min_timepoint = 1000000\n",
    "\n",
    "for t in valid_trials:\n",
    "    print(t)\n",
    "    trial = scipy.io.loadmat(trace_dir + str(t) + '.mat')['ch_MUA']\n",
    "    timepoint, chan = trial.shape\n",
    "    if min_timepoint > timepoint:\n",
    "        min_timepoint = timepoint\n",
    "    \n",
    "    trial = trial[:min_timepoint, :]\n",
    "    trial = trial.reshape((1, min_timepoint, chan))\n",
    "    print(trial.shape)\n",
    "    if epochs.size == 0:\n",
    "        epochs = trial\n",
    "    else:\n",
    "        epochs = epochs[:, :min_timepoint, :]\n",
    "        epochs = np.concatenate((epochs, trial), axis = 0)\n",
    "    print(epochs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial, timepoint, channel = epochs.shape\n",
    "sparse_ind = [i*100 for i in range(timepoint//100) ]\n",
    "print(len(sparse_ind))\n",
    "sparse_epochs = epochs[:, sparse_ind, :]\n",
    "sparse_epochs = np.swapaxes(sparse_epochs,1,2)\n",
    "\n",
    "\n",
    "file = open(sparse_epoch_to_save, 'wb')\n",
    "np.save(file, sparse_epochs)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 32, 1033)\n",
      "[0, 2, 3, 7, 10, 14, 15, 20, 22, 23, 25, 27, 30, 31]\n",
      "(317, 18, 1033)\n"
     ]
    }
   ],
   "source": [
    "sparse_epochs = np.load('sparse_ANM210861_20130702.npy')\n",
    "labels = np.load('ANM210861_20130702_labels.npy')\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "# ind = np.linspace(0,3443-1, num = 300)\n",
    "# ind = [int(np.floor(i)) for i in ind]\n",
    "# sparse_epochs = sparse_epochs[:,:, ind]\n",
    "# print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "\n",
    "#get rid of flat channel\n",
    "\n",
    "trial, channel,timepoint = sparse_epochs.shape\n",
    "flat_chan = [i for i in range(channel) if sparse_epochs[0,i,0] == 0]\n",
    "good_chan = [i for i in range(0,32) if i not in flat_chan]\n",
    "print(flat_chan)\n",
    "sparse_epochs = sparse_epochs[:,good_chan,:]\n",
    "print(sparse_epochs.shape)\n",
    "\n",
    "\n",
    "#standardize signal based on each channel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "epochs_std = copy.copy(sparse_epochs)\n",
    "sample_num, chan_num, timepoint = sparse_epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = sparse_epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    epochs_std[:,c,:] = chan_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on single session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 18, 18)\n",
      "0.924901185770751\n",
      "(253, 18, 18)\n",
      "0.9209486166007905\n",
      "(253, 18, 18)\n",
      "0.9288537549407114\n",
      "(253, 18, 18)\n",
      "0.9288537549407114\n",
      "(253, 18, 18)\n",
      "0.9169960474308301\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.932806324110672\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.9367588932806324\n",
      "(253, 18, 18)\n",
      "0.924901185770751\n",
      "Classification accuracy: 0.901563 / Chance level: 0.457413\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = epochs_std\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    print(cov_X_train.shape)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=sklearn.discriminant_analysis.LinearDiscriminantAnalysis())\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PCA across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "for n_component in range(30,31):\n",
    "    sparse_epochs_rs = sparse_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    pca.fit(sparse_epochs_rs)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    new_X = pca.transform(sparse_epochs_rs)\n",
    "\n",
    "    samples = new_X\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PCA across channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "for n_components in range(3,12):\n",
    "    samples = []\n",
    "    for s in range(sparse_epochs.shape[0]):\n",
    "        X = sparse_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((sparse_epochs.shape[0], -1))\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1]\n",
      "[1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1\n",
      " 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1]\n",
      "0.803030303030303\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
      " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0]\n",
      "[1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
      " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0]\n",
      "0.8787878787878788\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0]\n",
      "[1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0]\n",
      "0.8636363636363636\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
      " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1]\n",
      "[0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
      " 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1]\n",
      "0.7878787878787878\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0]\n",
      "[0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0]\n",
      "0.8484848484848485\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1]\n",
      "[1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1]\n",
      "0.8939393939393939\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1]\n",
      "0.803030303030303\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0]\n",
      "0.8181818181818182\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      "[0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      "0.8636363636363636\n",
      "(262, 19, 300)\n",
      "(262, 4)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
      " 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1]\n",
      "0.7878787878787878\n",
      "Classification accuracy: 0.834848 / Chance level: 0.509146\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from mne import Epochs, pick_types, find_events\n",
    "from mne.channels import read_layout\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "\n",
    "#you need to make sure there is no flat channel! otherwise covariance matrix is not positive definite\n",
    "\n",
    "scores = []\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in cv.split(epochs_std):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_std[train_idx]\n",
    "    X_test = epochs_std[test_idx]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False, cov_est = 'epoch', cov_method_params= 'shrinkage')\n",
    "    new_epochs = csp.fit_transform(X_train,  y_train)\n",
    "    print(new_epochs.shape)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(new_epochs, y_train)\n",
    "    \n",
    "#     logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#     logistic.fit(new_epochs, y_train)\n",
    "    \n",
    "\n",
    "    y_pred_csp = csp.transform(X_test)\n",
    "    y_pred = lda.predict(y_pred_csp)\n",
    "#     y_predict = logistic.predict(y_pred_csp)\n",
    "    \n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aggregate data across session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(642, 32, 1033)\n",
      "(642,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "# sparse_files = ['sparse_ANM210861_20130701.npy', 'sparse_ANM210861_20130702.npy', \n",
    "#                'sparse_ANM210861_20130703.npy']\n",
    "\n",
    "# label_files = ['ANM210861_20130701_labels.npy', 'ANM210861_20130702_labels.npy',\n",
    "#               'ANM210861_20130703_labels.npy']\n",
    "\n",
    "sparse_files = ['sparse_ANM210862_20130626.npy', 'sparse_ANM210862_20130627.npy', \n",
    "               'sparse_ANM210862_20130628.npy']\n",
    "\n",
    "label_files = ['ANM210862_20130626_labels.npy', 'ANM210862_20130627_labels.npy',\n",
    "              'ANM210862_20130628_labels.npy']\n",
    "\n",
    "\n",
    "agg_spares_epochs = np.load(sparse_files[0])\n",
    "agg_labels = np.load(label_files[0])\n",
    "# ind = np.linspace(0,3443-1, num = 1033)\n",
    "# ind = [int(np.floor(i)) for i in ind]\n",
    "# agg_spares_epochs = agg_spares_epochs[:,:, ind]\n",
    "\n",
    "\n",
    "for i in range(1,3):\n",
    "    new_epochs =  np.load(sparse_files[i])\n",
    "    agg_spares_epochs = np.concatenate((agg_spares_epochs, new_epochs), axis = 0)\n",
    "    new_label = np.load(label_files[i])\n",
    "    agg_labels = np.concatenate((agg_labels, new_label), axis = 0)\n",
    "\n",
    "print(agg_spares_epochs.shape)\n",
    "print(agg_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "agg_epochs_std = copy.copy(agg_spares_epochs)\n",
    "sample_num, chan_num, timepoint = agg_spares_epochs.shape\n",
    "for c in range(chan_num):\n",
    "    original_timepoints = agg_spares_epochs[:,c,:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(original_timepoints)\n",
    "    chan_std = scaler.transform(original_timepoints)\n",
    "    agg_epochs_std[:,c,:] = chan_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using Riemannian based classifier on mix dataset across session from same subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9941520467836257\n",
      "(513, 32, 1033)\n",
      "1.0\n",
      "(513, 32, 1033)\n",
      "1.0\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "Classification accuracy: 0.748837 / Chance level: 0.526480\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=sklearn.discriminant_analysis.LinearDiscriminantAnalysis())\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100))\n",
    "     \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=LogisticRegression())\n",
    "\n",
    "    TSclassifier.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = TSclassifier.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(TSclassifier.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction using PCA + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "trial,_,_ = agg_spares_epochs.shape\n",
    "for n_component in range(10,30):\n",
    "    epochs_reshaped = agg_spares_epochs.reshape((trial, -1))\n",
    "    pca = PCA(n_components=n_component)\n",
    "    samples = pca.fit_transform(epochs_reshaped)\n",
    "    total_variance = np.asarray(pca.explained_variance_ratio_)\n",
    "    print(sum(total_variance))\n",
    "    \n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "#         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "#         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prediction Using PCA + logistic across channels on aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_components in range(3,30):\n",
    "    samples = []\n",
    "    for s in range(agg_spares_epochs.shape[0]):\n",
    "        X = agg_spares_epochs[s,:,:]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        new_X = pca.transform(X)\n",
    "        samples.append(new_X)\n",
    "    samples = np.asarray(samples)\n",
    "    print(samples.shape)\n",
    "    samples = samples.reshape((agg_spares_epochs.shape[0], -1))\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "\n",
    "    labels = agg_labels\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(samples):\n",
    "        y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "        X_train = samples[train_idx]\n",
    "        X_test = samples[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #         logistic = linear_model.LogisticRegression(C = 1e-5)\n",
    "    #         logistic.fit(X_train, y_train)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #         y_predict = logistic.predict(X_test)\n",
    "        y_predict = lda.predict(X_test)\n",
    "        scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "    class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "    print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                              class_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting With MDM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 19, 300)\n",
      "0.7824427480916031\n",
      "(262, 19, 300)\n",
      "0.7977099236641222\n",
      "(262, 19, 300)\n",
      "0.7862595419847328\n",
      "(262, 19, 300)\n",
      "0.7938931297709924\n",
      "(262, 19, 300)\n",
      "0.8053435114503816\n",
      "(262, 19, 300)\n",
      "0.7786259541984732\n",
      "(262, 19, 300)\n",
      "0.7900763358778626\n",
      "(262, 19, 300)\n",
      "0.7633587786259542\n",
      "(262, 19, 300)\n",
      "0.7938931297709924\n",
      "(262, 19, 300)\n",
      "0.8091603053435115\n",
      "Classification accuracy: 0.751515 / Chance level: 0.509146\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = sparse_epochs\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    \n",
    "    MDMclassifier = pyriemann.classification.MDM(metric='riemann')\n",
    "    clf = AdaBoostClassifier(n_estimators=4, base_estimator=MDMclassifier,learning_rate=0.7)\n",
    "    \n",
    "    \n",
    "    clf.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = clf.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(clf.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "#MDM with no boosting: 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "0.9961013645224172\n",
      "(513, 32, 1033)\n",
      "0.9941520467836257\n",
      "(513, 32, 1033)\n",
      "0.9980506822612085\n",
      "(513, 32, 1033)\n",
      "1.0\n",
      "(513, 32, 1033)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f78102d6908b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTSclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTangentSpace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\tangentspace.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_reference_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         self.reference_ = mean_covariance(X, metric=self.metric,\n\u001b[1;32m--> 169\u001b[1;33m                                           sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtangent_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreference_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\utils\\mean.py\u001b[0m in \u001b[0;36mmean_covariance\u001b[1;34m(covmats, metric, sample_weight, *args)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_methods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyriemann\\utils\\mean.py\u001b[0m in \u001b[0;36mmean_riemann\u001b[1;34m(covmats, tol, maxiter, init, sample_weight)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCm12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovmats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCm12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mJ\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import pyriemann.estimation\n",
    "import pyriemann.classification\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import ShuffleSplit  \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    \n",
    "#     TSclassifier = pyriemann.classification.TSclassifier(metric='riemann')\n",
    "    TSclassifier = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=5, base_estimator=TSclassifier,learning_rate=1)\n",
    "#     \n",
    "    clf.fit(cov_X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_predict = clf.predict(cov_X_test)\n",
    "    print(sklearn.metrics.accuracy_score(clf.predict(cov_X_train), y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n",
    "\n",
    "#MDM with no boosting: 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "0.8966861598440545\n",
      "(513, 32, 1033)\n",
      "0.9161793372319688\n",
      "(513, 32, 1033)\n",
      "0.8927875243664717\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.8732943469785575\n",
      "(513, 32, 1033)\n",
      "0.9181286549707602\n",
      "(513, 32, 1033)\n",
      "0.8947368421052632\n",
      "(513, 32, 1033)\n",
      "0.9122807017543859\n",
      "Classification accuracy: 0.730233 / Chance level: 0.526480\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    n_samples, _,_ = cov_X_train.shape\n",
    "    ind_set_1 = np.random.randint(0, n_samples, size = n_samples//3 * 2)\n",
    "    ind_set_2 = np.random.randint(0, n_samples, size = n_samples//3 * 2)\n",
    "    ind_set_3 = np.random.randint(0, n_samples, size = n_samples//3*2)\n",
    "\n",
    "    clf_1 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "    clf_2 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "    clf_3 = pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True))\n",
    "\n",
    "    clf_1.fit(cov_X_train[ind_set_1,:,:], y_train[ind_set_1])\n",
    "    clf_2.fit(cov_X_train[ind_set_2,:,:], y_train[ind_set_2])\n",
    "    clf_3.fit(cov_X_train[ind_set_3,:,:], y_train[ind_set_3])\n",
    "\n",
    "    y_predict = scipy.stats.mode([clf_1.predict(cov_X_test), clf_2.predict(cov_X_test), clf_3.predict(cov_X_test)], axis=0).mode[0]\n",
    "    print(sklearn.metrics.accuracy_score(scipy.stats.mode([clf_1.predict(cov_X_train), clf_2.predict(cov_X_train), clf_3.predict(cov_X_train)], axis=0).mode[0], y_train))\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 32, 1033)\n",
      "[0.7131782945736435]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356, 0.7131782945736435]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356, 0.7131782945736435, 0.7441860465116279]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356, 0.7131782945736435, 0.7441860465116279, 0.7674418604651163]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356, 0.7131782945736435, 0.7441860465116279, 0.7674418604651163, 0.7751937984496124]\n",
      "(513, 32, 1033)\n",
      "[0.7131782945736435, 0.7751937984496124, 0.7286821705426356, 0.7131782945736435, 0.7441860465116279, 0.7674418604651163, 0.7751937984496124, 0.7596899224806202]\n",
      "(513, 32, 1033)\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "epochs_data = agg_epochs_std\n",
    "labels = agg_labels\n",
    "\n",
    "n_estimator = 51\n",
    "subset_size = n_samples//2\n",
    "\n",
    "for train_idx, test_idx in cv.split(epochs_data):\n",
    "    y_train, y_test = np.asarray(labels)[train_idx], np.asarray(labels)[test_idx]\n",
    "    X_train = epochs_data[train_idx]\n",
    "    X_test = epochs_data[test_idx]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    cov =  pyriemann.estimation.Covariances('lwf')\n",
    "    cov_X_train = cov.transform(X_train)\n",
    "    cov_X_test = cov.transform(X_test)\n",
    "    \n",
    "    n_samples, _,_ = cov_X_train.shape\n",
    "    \n",
    "    ind_sets = [np.random.randint(0, n_samples, size = subset_size) for i in range(n_estimator)]\n",
    "    \n",
    "    clfs = [pyriemann.classification.TSclassifier(metric='riemann', clf=SVC(kernel='rbf', random_state=0, gamma= 0.03, C=100, probability=True)) for i in range(n_estimator)]\n",
    "\n",
    "    for i in range(n_estimator):\n",
    "        clfs[i].fit(cov_X_train[ind_sets[i],:,:], y_train[ind_sets[i]])\n",
    "    \n",
    "    y_predict = [clfs[i].predict(cov_X_test) for i in range(n_estimator)]\n",
    "    y_predict = scipy.stats.mode(y_predict, axis=0).mode[0]\n",
    "    \n",
    "\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_predict, y_test))\n",
    "    print(scores)\n",
    "\n",
    "class_balance = np.mean(np.asarray(labels) == np.asarray(labels)[0])\n",
    "print(\"Classification accuracy: %f / Chance level: %f\" % (np.mean(scores),\n",
    "                                                          class_balance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "ind_sets = [np.random.randint(0, n_samples, size = n_samples//3 * 2) for i in range(5)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
